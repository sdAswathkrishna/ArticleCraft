{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medium Articles NLP Analysis\n",
    "\n",
    "This notebook performs Natural Language Processing on a large dataset of Medium articles. Due to the size of the dataset, we'll implement techniques to handle memory constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aswat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aswat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aswat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "# NLP libraries\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# For memory usage tracking\n",
    "import psutil\n",
    "\n",
    "# # Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "def print_memory_usage(message=\"\"):\n",
    "    \"\"\"Print memory usage with optional message\"\"\"\n",
    "    memory_mb = get_memory_usage()\n",
    "    print(f\"{message} - Memory usage: {memory_mb:.2f} MB\")\n",
    "    \n",
    "# Create directory for intermediate results if it doesn't exist\n",
    "if not os.path.exists('intermediate_data'):\n",
    "    os.makedirs('intermediate_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from medium_articles.csv...\n",
      "Loaded chunk 1 with 10000 rows\n",
      " - Memory usage: 341.80 MB\n",
      "Loaded chunk 2 with 10000 rows\n",
      " - Memory usage: 455.39 MB\n",
      "Loaded chunk 3 with 10000 rows\n",
      " - Memory usage: 562.93 MB\n",
      "Loaded chunk 4 with 10000 rows\n",
      " - Memory usage: 674.85 MB\n",
      "Loaded chunk 5 with 10000 rows\n",
      " - Memory usage: 784.25 MB\n",
      "Loaded chunk 6 with 10000 rows\n",
      " - Memory usage: 895.07 MB\n",
      "Loaded chunk 7 with 10000 rows\n",
      " - Memory usage: 1002.96 MB\n",
      "Loaded chunk 8 with 10000 rows\n",
      " - Memory usage: 1100.00 MB\n",
      "Loaded chunk 9 with 10000 rows\n",
      " - Memory usage: 1198.41 MB\n",
      "Loaded chunk 10 with 10000 rows\n",
      " - Memory usage: 1295.29 MB\n",
      "Loaded chunk 11 with 10000 rows\n",
      " - Memory usage: 1392.66 MB\n",
      "Loaded chunk 12 with 10000 rows\n",
      " - Memory usage: 1489.96 MB\n",
      "Loaded chunk 13 with 10000 rows\n",
      " - Memory usage: 1585.64 MB\n",
      "Loaded chunk 14 with 10000 rows\n",
      " - Memory usage: 1685.90 MB\n",
      "Loaded chunk 15 with 10000 rows\n",
      " - Memory usage: 1801.27 MB\n",
      "Loaded chunk 16 with 10000 rows\n",
      " - Memory usage: 1919.40 MB\n",
      "Loaded chunk 17 with 10000 rows\n",
      " - Memory usage: 2040.50 MB\n",
      "Loaded chunk 18 with 10000 rows\n",
      " - Memory usage: 2167.40 MB\n",
      "Loaded chunk 19 with 10000 rows\n",
      " - Memory usage: 2285.49 MB\n",
      "Loaded chunk 20 with 2368 rows\n",
      " - Memory usage: 2306.44 MB\n",
      "Total rows loaded: 192368\n"
     ]
    }
   ],
   "source": [
    "# We'll use chunking to handle large files\n",
    "\n",
    "def load_dataset_in_chunks(file_path, chunk_size=10000):\n",
    "    \"\"\"Load large Csv dataset in chunks\"\"\"\n",
    "    print(f\"Loading dataset from {file_path}...\")\n",
    "    \n",
    "    chunks = []\n",
    "    total_rows = 0\n",
    "    \n",
    "    # Use chunksize to load in batches\n",
    "    for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):\n",
    "        print(f\"Loaded chunk {i+1} with {len(chunk)} rows\")\n",
    "        chunks.append(chunk)\n",
    "        total_rows += len(chunk)\n",
    "        print_memory_usage()\n",
    "    \n",
    "    print(f\"Total rows loaded: {total_rows}\")\n",
    "    return chunks\n",
    "\n",
    "# Replace 'medium_articles.json' with your actual file path\n",
    "file_path = 'medium_articles.csv'  \n",
    "data_chunks = load_dataset_in_chunks(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data structure:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>authors</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mental Note Vol. 24</td>\n",
       "      <td>Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...</td>\n",
       "      <td>https://medium.com/invisible-illness/mental-no...</td>\n",
       "      <td>['Ryan Fan']</td>\n",
       "      <td>2020-12-26 03:38:10.479000+00:00</td>\n",
       "      <td>['Mental Health', 'Health', 'Psychology', 'Sci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Your Brain On Coronavirus</td>\n",
       "      <td>Your Brain On Coronavirus\\n\\nA guide to the cu...</td>\n",
       "      <td>https://medium.com/age-of-awareness/how-the-pa...</td>\n",
       "      <td>['Simon Spichak']</td>\n",
       "      <td>2020-09-23 22:10:17.126000+00:00</td>\n",
       "      <td>['Mental Health', 'Coronavirus', 'Science', 'P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mind Your Nose</td>\n",
       "      <td>Mind Your Nose\\n\\nHow smell training can chang...</td>\n",
       "      <td>https://medium.com/neodotlife/mind-your-nose-f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2020-10-10 20:17:37.132000+00:00</td>\n",
       "      <td>['Biotechnology', 'Neuroscience', 'Brain', 'We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The 4 Purposes of Dreams</td>\n",
       "      <td>Passionate about the synergy between science a...</td>\n",
       "      <td>https://medium.com/science-for-real/the-4-purp...</td>\n",
       "      <td>['Eshan Samaranayake']</td>\n",
       "      <td>2020-12-21 16:05:19.524000+00:00</td>\n",
       "      <td>['Health', 'Neuroscience', 'Mental Health', 'P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Surviving a Rod Through the Head</td>\n",
       "      <td>You’ve heard of him, haven’t you? Phineas Gage...</td>\n",
       "      <td>https://medium.com/live-your-life-on-purpose/s...</td>\n",
       "      <td>['Rishav Sinha']</td>\n",
       "      <td>2020-02-26 00:01:01.576000+00:00</td>\n",
       "      <td>['Brain', 'Health', 'Development', 'Psychology...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              title  \\\n",
       "0               Mental Note Vol. 24   \n",
       "1         Your Brain On Coronavirus   \n",
       "2                    Mind Your Nose   \n",
       "3          The 4 Purposes of Dreams   \n",
       "4  Surviving a Rod Through the Head   \n",
       "\n",
       "                                                text  \\\n",
       "0  Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...   \n",
       "1  Your Brain On Coronavirus\\n\\nA guide to the cu...   \n",
       "2  Mind Your Nose\\n\\nHow smell training can chang...   \n",
       "3  Passionate about the synergy between science a...   \n",
       "4  You’ve heard of him, haven’t you? Phineas Gage...   \n",
       "\n",
       "                                                 url                 authors  \\\n",
       "0  https://medium.com/invisible-illness/mental-no...            ['Ryan Fan']   \n",
       "1  https://medium.com/age-of-awareness/how-the-pa...       ['Simon Spichak']   \n",
       "2  https://medium.com/neodotlife/mind-your-nose-f...                      []   \n",
       "3  https://medium.com/science-for-real/the-4-purp...  ['Eshan Samaranayake']   \n",
       "4  https://medium.com/live-your-life-on-purpose/s...        ['Rishav Sinha']   \n",
       "\n",
       "                          timestamp  \\\n",
       "0  2020-12-26 03:38:10.479000+00:00   \n",
       "1  2020-09-23 22:10:17.126000+00:00   \n",
       "2  2020-10-10 20:17:37.132000+00:00   \n",
       "3  2020-12-21 16:05:19.524000+00:00   \n",
       "4  2020-02-26 00:01:01.576000+00:00   \n",
       "\n",
       "                                                tags  \n",
       "0  ['Mental Health', 'Health', 'Psychology', 'Sci...  \n",
       "1  ['Mental Health', 'Coronavirus', 'Science', 'P...  \n",
       "2  ['Biotechnology', 'Neuroscience', 'Brain', 'We...  \n",
       "3  ['Health', 'Neuroscience', 'Mental Health', 'P...  \n",
       "4  ['Brain', 'Health', 'Development', 'Psychology...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the first chunk to understand the data structure\n",
    "sample_df = data_chunks[0]\n",
    "\n",
    "# Display basic information\n",
    "print(\"Sample data structure:\")\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataframe info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   title      9999 non-null   object\n",
      " 1   text       10000 non-null  object\n",
      " 2   url        10000 non-null  object\n",
      " 3   authors    10000 non-null  object\n",
      " 4   timestamp  10000 non-null  object\n",
      " 5   tags       10000 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 468.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Display dataframe info\n",
    "print(\"\\nDataframe info:\")\n",
    "sample_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descriptive statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>authors</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9999</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>9976</td>\n",
       "      <td>9912</td>\n",
       "      <td>10000</td>\n",
       "      <td>5652</td>\n",
       "      <td>9988</td>\n",
       "      <td>9842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Our FAQs</td>\n",
       "      <td>in In Fitness And In Health</td>\n",
       "      <td>https://medium.com/brian-berg/myfitnesspal-re-...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2020-11-19 01:16:58.476000+00:00</td>\n",
       "      <td>['Startup']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>691</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           title                         text  \\\n",
       "count       9999                        10000   \n",
       "unique      9976                         9912   \n",
       "top     Our FAQs  in In Fitness And In Health   \n",
       "freq           8                           21   \n",
       "\n",
       "                                                      url authors  \\\n",
       "count                                               10000   10000   \n",
       "unique                                              10000    5652   \n",
       "top     https://medium.com/brian-berg/myfitnesspal-re-...      []   \n",
       "freq                                                    1     691   \n",
       "\n",
       "                               timestamp         tags  \n",
       "count                              10000        10000  \n",
       "unique                              9988         9842  \n",
       "top     2020-11-19 01:16:58.476000+00:00  ['Startup']  \n",
       "freq                                   8            9  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display statistics for numeric columns (if any)\n",
    "print(\"\\nDescriptive statistics:\")\n",
    "sample_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows across all chunks: 192368\n",
      "\n",
      "Null values per column:\n",
      "title: 5 nulls (0.00%)\n",
      "text: 0 nulls (0.00%)\n",
      "url: 0 nulls (0.00%)\n",
      "authors: 0 nulls (0.00%)\n",
      "timestamp: 2 nulls (0.00%)\n",
      "tags: 0 nulls (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Check for column data types and null values across all chunks\n",
    "def analyze_all_chunks(chunks):\n",
    "    total_rows = 0\n",
    "    null_counts = {}\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        total_rows += len(chunk)\n",
    "        \n",
    "        # Count nulls in this chunk\n",
    "        chunk_nulls = chunk.isnull().sum()\n",
    "        \n",
    "        # Update total null counts\n",
    "        for col in chunk.columns:\n",
    "            if col not in null_counts:\n",
    "                null_counts[col] = 0\n",
    "            null_counts[col] += chunk_nulls[col]\n",
    "    \n",
    "    print(f\"Total rows across all chunks: {total_rows}\")\n",
    "    print(\"\\nNull values per column:\")\n",
    "    for col, count in null_counts.items():\n",
    "        percentage = (count / total_rows) * 100\n",
    "        print(f\"{col}: {count} nulls ({percentage:.2f}%)\")\n",
    "\n",
    "analyze_all_chunks(data_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting data types for chunk 1...\n",
      "Saved converted chunk to intermediate_data/converted_chunk_0.pkl\n",
      " - Memory usage: 2369.83 MB\n",
      "Converting data types for chunk 2...\n",
      "Saved converted chunk to intermediate_data/converted_chunk_1.pkl\n",
      " - Memory usage: 2428.20 MB\n",
      "Converting data types for chunk 3...\n",
      "Saved converted chunk to intermediate_data/converted_chunk_2.pkl\n",
      " - Memory usage: 2479.93 MB\n",
      "Converting data types for chunk 4...\n",
      "Saved converted chunk to intermediate_data/converted_chunk_3.pkl\n",
      " - Memory usage: 2530.85 MB\n",
      "Converting data types for chunk 5...\n",
      "Saved converted chunk to intermediate_data/converted_chunk_4.pkl\n",
      " - Memory usage: 2582.06 MB\n",
      "Converting data types for chunk 6...\n",
      "Saved converted chunk to intermediate_data/converted_chunk_5.pkl\n",
      " - Memory usage: 2632.87 MB\n",
      "Converting data types for chunk 7...\n",
      "Saved converted chunk to intermediate_data/converted_chunk_6.pkl\n",
      " - Memory usage: 2684.05 MB\n",
      "Converting data types for chunk 8...\n",
      "Saved converted chunk to intermediate_data/converted_chunk_7.pkl\n",
      " - Memory usage: 2726.67 MB\n",
      "Converting data types for chunk 9...\n",
      "Saved converted chunk to intermediate_data/converted_chunk_8.pkl\n",
      " - Memory usage: 2772.28 MB\n",
      "Converting data types for chunk 10...\n",
      "Saved converted chunk to intermediate_data/converted_chunk_9.pkl\n",
      " - Memory usage: 2817.43 MB\n",
      "Converting data types for chunk 11...\n",
      "Saved converted chunk to intermediate_data/converted_chunk_10.pkl\n",
      " - Memory usage: 2859.86 MB\n",
      "Converting data types for chunk 12...\n",
      "Saved converted chunk to intermediate_data/converted_chunk_11.pkl\n",
      " - Memory usage: 2906.47 MB\n",
      "Converting data types for chunk 13...\n",
      "Saved converted chunk to intermediate_data/converted_chunk_12.pkl\n",
      " - Memory usage: 2950.41 MB\n",
      "Converting data types for chunk 14...\n",
      "Saved converted chunk to intermediate_data/converted_chunk_13.pkl\n",
      " - Memory usage: 2997.68 MB\n",
      "Converting data types for chunk 15...\n",
      "Saved converted chunk to intermediate_data/converted_chunk_14.pkl\n",
      " - Memory usage: 3049.41 MB\n",
      "Converting data types for chunk 16...\n",
      "Saved converted chunk to intermediate_data/converted_chunk_15.pkl\n",
      " - Memory usage: 3105.10 MB\n",
      "Converting data types for chunk 17...\n",
      "Saved converted chunk to intermediate_data/converted_chunk_16.pkl\n",
      " - Memory usage: 3161.84 MB\n",
      "Converting data types for chunk 18...\n",
      "Saved converted chunk to intermediate_data/converted_chunk_17.pkl\n",
      " - Memory usage: 3218.75 MB\n",
      "Converting data types for chunk 19...\n",
      "Saved converted chunk to intermediate_data/converted_chunk_18.pkl\n",
      " - Memory usage: 3273.20 MB\n",
      "Converting data types for chunk 20...\n",
      "Saved converted chunk to intermediate_data/converted_chunk_19.pkl\n",
      " - Memory usage: 3283.11 MB\n",
      "After clearing data chunks - Memory usage: 546.40 MB\n"
     ]
    }
   ],
   "source": [
    "import ast  # Add this import at the top\n",
    "\n",
    "def convert_data_types(df):\n",
    "    \"\"\"Convert data types for timestamp, authors, and tags\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    if 'timestamp' in df_copy.columns:\n",
    "        df_copy['timestamp'] = pd.to_datetime(df_copy['timestamp'], errors='coerce')\n",
    "\n",
    "    # Helper function to safely evaluate strings into lists\n",
    "    def safe_parse_list(x):\n",
    "        if isinstance(x, str):\n",
    "            try:\n",
    "                return ast.literal_eval(x)\n",
    "            except (ValueError, SyntaxError):\n",
    "                return []\n",
    "        elif isinstance(x, list):\n",
    "            return x\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    # Ensure authors and tags are proper lists\n",
    "    if 'authors' in df_copy.columns:\n",
    "        df_copy['authors'] = df_copy['authors'].apply(safe_parse_list)\n",
    "\n",
    "    if 'tags' in df_copy.columns:\n",
    "        df_copy['tags'] = df_copy['tags'].apply(safe_parse_list)\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Process each chunk and save as intermediate file\n",
    "for i, chunk in enumerate(data_chunks):\n",
    "    print(f\"Converting data types for chunk {i+1}...\")\n",
    "    converted_chunk = convert_data_types(chunk)\n",
    "    \n",
    "    # Save intermediate result\n",
    "    intermediate_file = f'intermediate_data/converted_chunk_{i}.pkl'\n",
    "    converted_chunk.to_pickle(intermediate_file)\n",
    "    print(f\"Saved converted chunk to {intermediate_file}\")\n",
    "    print_memory_usage()\n",
    "\n",
    "# Clear memory\n",
    "del data_chunks\n",
    "gc.collect()\n",
    "print_memory_usage(\"After clearing data chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading intermediate_data/converted_chunk_0.pkl...\n",
      "Cleaning chunk 1...\n",
      "Saved cleaned chunk to intermediate_data/cleaned_chunk_0.pkl\n",
      " - Memory usage: 734.79 MB\n",
      "Loading intermediate_data/converted_chunk_1.pkl...\n",
      "Cleaning chunk 2...\n",
      "Saved cleaned chunk to intermediate_data/cleaned_chunk_1.pkl\n",
      " - Memory usage: 726.70 MB\n",
      "Loading intermediate_data/converted_chunk_2.pkl...\n",
      "Cleaning chunk 3...\n",
      "Saved cleaned chunk to intermediate_data/cleaned_chunk_2.pkl\n",
      " - Memory usage: 713.14 MB\n",
      "Loading intermediate_data/converted_chunk_3.pkl...\n",
      "Cleaning chunk 4...\n",
      "Saved cleaned chunk to intermediate_data/cleaned_chunk_3.pkl\n",
      " - Memory usage: 718.69 MB\n",
      "Loading intermediate_data/converted_chunk_4.pkl...\n",
      "Cleaning chunk 5...\n",
      "Saved cleaned chunk to intermediate_data/cleaned_chunk_4.pkl\n",
      " - Memory usage: 717.50 MB\n",
      "Loading intermediate_data/converted_chunk_5.pkl...\n",
      "Cleaning chunk 6...\n",
      "Saved cleaned chunk to intermediate_data/cleaned_chunk_5.pkl\n",
      " - Memory usage: 714.77 MB\n",
      "Loading intermediate_data/converted_chunk_6.pkl...\n",
      "Cleaning chunk 7...\n",
      "Saved cleaned chunk to intermediate_data/cleaned_chunk_6.pkl\n",
      " - Memory usage: 712.88 MB\n",
      "Loading intermediate_data/converted_chunk_7.pkl...\n",
      "Cleaning chunk 8...\n",
      "Saved cleaned chunk to intermediate_data/cleaned_chunk_7.pkl\n",
      " - Memory usage: 691.46 MB\n",
      "Loading intermediate_data/converted_chunk_8.pkl...\n",
      "Cleaning chunk 9...\n",
      "Saved cleaned chunk to intermediate_data/cleaned_chunk_8.pkl\n",
      " - Memory usage: 699.02 MB\n",
      "Loading intermediate_data/converted_chunk_9.pkl...\n",
      "Cleaning chunk 10...\n",
      "Saved cleaned chunk to intermediate_data/cleaned_chunk_9.pkl\n",
      " - Memory usage: 688.71 MB\n",
      "Loading intermediate_data/converted_chunk_10.pkl...\n",
      "Cleaning chunk 11...\n",
      "Saved cleaned chunk to intermediate_data/cleaned_chunk_10.pkl\n",
      " - Memory usage: 688.34 MB\n",
      "Loading intermediate_data/converted_chunk_11.pkl...\n",
      "Cleaning chunk 12...\n",
      "Saved cleaned chunk to intermediate_data/cleaned_chunk_11.pkl\n",
      " - Memory usage: 691.01 MB\n",
      "Loading intermediate_data/converted_chunk_12.pkl...\n",
      "Cleaning chunk 13...\n",
      "Saved cleaned chunk to intermediate_data/cleaned_chunk_12.pkl\n",
      " - Memory usage: 689.06 MB\n",
      "Loading intermediate_data/converted_chunk_13.pkl...\n",
      "Cleaning chunk 14...\n",
      "Saved cleaned chunk to intermediate_data/cleaned_chunk_13.pkl\n",
      " - Memory usage: 702.88 MB\n",
      "Loading intermediate_data/converted_chunk_14.pkl...\n",
      "Cleaning chunk 15...\n",
      "Saved cleaned chunk to intermediate_data/cleaned_chunk_14.pkl\n",
      " - Memory usage: 725.93 MB\n",
      "Loading intermediate_data/converted_chunk_15.pkl...\n",
      "Cleaning chunk 16...\n",
      "Saved cleaned chunk to intermediate_data/cleaned_chunk_15.pkl\n",
      " - Memory usage: 730.84 MB\n",
      "Loading intermediate_data/converted_chunk_16.pkl...\n",
      "Cleaning chunk 17...\n",
      "Saved cleaned chunk to intermediate_data/cleaned_chunk_16.pkl\n",
      " - Memory usage: 734.35 MB\n",
      "Loading intermediate_data/converted_chunk_17.pkl...\n",
      "Cleaning chunk 18...\n",
      "Saved cleaned chunk to intermediate_data/cleaned_chunk_17.pkl\n",
      " - Memory usage: 751.20 MB\n",
      "Loading intermediate_data/converted_chunk_18.pkl...\n",
      "Cleaning chunk 19...\n",
      "Saved cleaned chunk to intermediate_data/cleaned_chunk_18.pkl\n",
      " - Memory usage: 755.20 MB\n",
      "Loading intermediate_data/converted_chunk_19.pkl...\n",
      "Cleaning chunk 20...\n",
      "Saved cleaned chunk to intermediate_data/cleaned_chunk_19.pkl\n",
      " - Memory usage: 578.95 MB\n",
      "After cleaning all chunks - Memory usage: 578.95 MB\n"
     ]
    }
   ],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"Clean data by removing nulls, filling empty lists, and cleaning text\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Remove rows with null values in critical columns\n",
    "    critical_cols = ['title', 'text', 'url']\n",
    "    df_clean = df_clean.dropna(subset=critical_cols)\n",
    "    \n",
    "    # Replace empty lists\n",
    "    if 'authors' in df_clean.columns:\n",
    "        df_clean['authors'] = df_clean['authors'].apply(lambda x: ['Unknown'] if not x else x)\n",
    "    \n",
    "    if 'tags' in df_clean.columns:\n",
    "        df_clean['tags'] = df_clean['tags'].apply(lambda x: ['Untagged'] if not x else x)\n",
    "    \n",
    "    # Clean text columns\n",
    "    def clean_text(text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        \n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()    \n",
    "        \n",
    "        # Remove newline characters\n",
    "        text = re.sub(r'\\n+', ' ', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    # Apply text cleaning to title and text columns\n",
    "    if 'title' in df_clean.columns:\n",
    "        df_clean['clean_title'] = df_clean['title'].apply(clean_text)\n",
    "    \n",
    "    if 'text' in df_clean.columns:\n",
    "        df_clean['clean_text'] = df_clean['text'].apply(clean_text)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Process each converted chunk and save as intermediate file\n",
    "num_chunks = len([f for f in os.listdir('intermediate_data') if f.startswith('converted_chunk_')])\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    # Load the converted chunk\n",
    "    file_path = f'intermediate_data/converted_chunk_{i}.pkl'\n",
    "    print(f\"Loading {file_path}...\")\n",
    "    chunk = pd.read_pickle(file_path)\n",
    "    \n",
    "    # Clean the chunk\n",
    "    print(f\"Cleaning chunk {i+1}...\")\n",
    "    cleaned_chunk = clean_data(chunk)\n",
    "    \n",
    "    # Save cleaned chunk\n",
    "    cleaned_file = f'intermediate_data/cleaned_chunk_{i}.pkl'\n",
    "    cleaned_chunk.to_pickle(cleaned_file)\n",
    "    print(f\"Saved cleaned chunk to {cleaned_file}\")\n",
    "    print_memory_usage()\n",
    "    \n",
    "    # Free memory\n",
    "    del chunk, cleaned_chunk\n",
    "    gc.collect()\n",
    "\n",
    "print_memory_usage(\"After cleaning all chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. NLP Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading intermediate_data/cleaned_chunk_0.pkl...\n",
      "Preprocessing chunk 1 for NLP...\n",
      "Processing text column...\n",
      "Processing batch 1...\n",
      "After processing batch 1 - Memory usage: 800.41 MB\n",
      "Processing batch 2...\n",
      "Processing batch 3...\n",
      "Processing batch 4...\n",
      "Processing batch 5...\n",
      "Processing batch 6...\n",
      "Processing batch 7...\n",
      "Processing batch 8...\n",
      "Processing batch 9...\n",
      "Processing batch 10...\n",
      "Processing title column...\n",
      "Saved NLP preprocessed chunk to intermediate_data/nlp_chunk_0.pkl\n",
      " - Memory usage: 1421.54 MB\n",
      "Loading intermediate_data/cleaned_chunk_1.pkl...\n",
      "Preprocessing chunk 2 for NLP...\n",
      "Processing text column...\n",
      "Processing batch 1...\n",
      "After processing batch 1 - Memory usage: 900.59 MB\n",
      "Processing batch 2...\n",
      "Processing batch 3...\n",
      "Processing batch 4...\n",
      "Processing batch 5...\n",
      "Processing batch 6...\n",
      "Processing batch 7...\n",
      "Processing batch 8...\n",
      "Processing batch 9...\n",
      "Processing batch 10...\n",
      "Processing title column...\n",
      "Saved NLP preprocessed chunk to intermediate_data/nlp_chunk_1.pkl\n",
      " - Memory usage: 1383.76 MB\n",
      "Loading intermediate_data/cleaned_chunk_2.pkl...\n",
      "Preprocessing chunk 3 for NLP...\n",
      "Processing text column...\n",
      "Processing batch 1...\n",
      "After processing batch 1 - Memory usage: 895.30 MB\n",
      "Processing batch 2...\n",
      "Processing batch 3...\n",
      "Processing batch 4...\n",
      "Processing batch 5...\n",
      "Processing batch 6...\n",
      "Processing batch 7...\n",
      "Processing batch 8...\n",
      "Processing batch 9...\n",
      "Processing batch 10...\n",
      "Processing title column...\n",
      "Saved NLP preprocessed chunk to intermediate_data/nlp_chunk_2.pkl\n",
      " - Memory usage: 1335.77 MB\n",
      "Loading intermediate_data/cleaned_chunk_3.pkl...\n",
      "Preprocessing chunk 4 for NLP...\n",
      "Processing text column...\n",
      "Processing batch 1...\n",
      "After processing batch 1 - Memory usage: 1060.78 MB\n",
      "Processing batch 2...\n",
      "Processing batch 3...\n",
      "Processing batch 4...\n",
      "Processing batch 5...\n",
      "Processing batch 6...\n",
      "Processing batch 7...\n",
      "Processing batch 8...\n",
      "Processing batch 9...\n",
      "Processing batch 10...\n",
      "Processing title column...\n",
      "Saved NLP preprocessed chunk to intermediate_data/nlp_chunk_3.pkl\n",
      " - Memory usage: 1358.89 MB\n",
      "Loading intermediate_data/cleaned_chunk_4.pkl...\n",
      "Preprocessing chunk 5 for NLP...\n",
      "Processing text column...\n",
      "Processing batch 1...\n",
      "After processing batch 1 - Memory usage: 929.77 MB\n",
      "Processing batch 2...\n",
      "Processing batch 3...\n",
      "Processing batch 4...\n",
      "Processing batch 5...\n",
      "Processing batch 6...\n",
      "Processing batch 7...\n",
      "Processing batch 8...\n",
      "Processing batch 9...\n",
      "Processing batch 10...\n",
      "Processing title column...\n",
      "Saved NLP preprocessed chunk to intermediate_data/nlp_chunk_4.pkl\n",
      " - Memory usage: 1353.86 MB\n",
      "Loading intermediate_data/cleaned_chunk_5.pkl...\n",
      "Preprocessing chunk 6 for NLP...\n",
      "Processing text column...\n",
      "Processing batch 1...\n",
      "After processing batch 1 - Memory usage: 1097.41 MB\n",
      "Processing batch 2...\n",
      "Processing batch 3...\n",
      "Processing batch 4...\n",
      "Processing batch 5...\n",
      "Processing batch 6...\n",
      "Processing batch 7...\n",
      "Processing batch 8...\n",
      "Processing batch 9...\n",
      "Processing batch 10...\n",
      "Processing title column...\n",
      "Saved NLP preprocessed chunk to intermediate_data/nlp_chunk_5.pkl\n",
      " - Memory usage: 1353.15 MB\n",
      "Loading intermediate_data/cleaned_chunk_6.pkl...\n",
      "Preprocessing chunk 7 for NLP...\n",
      "Processing text column...\n",
      "Processing batch 1...\n",
      "After processing batch 1 - Memory usage: 962.66 MB\n",
      "Processing batch 2...\n",
      "Processing batch 3...\n",
      "Processing batch 4...\n",
      "Processing batch 5...\n",
      "Processing batch 6...\n",
      "Processing batch 7...\n",
      "Processing batch 8...\n",
      "Processing batch 9...\n",
      "Processing batch 10...\n",
      "Processing title column...\n",
      "Saved NLP preprocessed chunk to intermediate_data/nlp_chunk_6.pkl\n",
      " - Memory usage: 1338.41 MB\n",
      "Loading intermediate_data/cleaned_chunk_7.pkl...\n",
      "Preprocessing chunk 8 for NLP...\n",
      "Processing text column...\n",
      "Processing batch 1...\n",
      "After processing batch 1 - Memory usage: 1113.77 MB\n",
      "Processing batch 2...\n",
      "Processing batch 3...\n",
      "Processing batch 4...\n",
      "Processing batch 5...\n",
      "Processing batch 6...\n",
      "Processing batch 7...\n",
      "Processing batch 8...\n",
      "Processing batch 9...\n",
      "Processing batch 10...\n",
      "Processing title column...\n",
      "Saved NLP preprocessed chunk to intermediate_data/nlp_chunk_7.pkl\n",
      " - Memory usage: 1281.11 MB\n",
      "Loading intermediate_data/cleaned_chunk_8.pkl...\n",
      "Preprocessing chunk 9 for NLP...\n",
      "Processing text column...\n",
      "Processing batch 1...\n",
      "After processing batch 1 - Memory usage: 963.68 MB\n",
      "Processing batch 2...\n",
      "Processing batch 3...\n",
      "Processing batch 4...\n",
      "Processing batch 5...\n",
      "Processing batch 6...\n",
      "Processing batch 7...\n",
      "Processing batch 8...\n",
      "Processing batch 9...\n",
      "Processing batch 10...\n",
      "Processing title column...\n",
      "Saved NLP preprocessed chunk to intermediate_data/nlp_chunk_8.pkl\n",
      " - Memory usage: 1301.38 MB\n",
      "Loading intermediate_data/cleaned_chunk_9.pkl...\n",
      "Preprocessing chunk 10 for NLP...\n",
      "Processing text column...\n",
      "Processing batch 1...\n",
      "After processing batch 1 - Memory usage: 1124.12 MB\n",
      "Processing batch 2...\n",
      "Processing batch 3...\n",
      "Processing batch 4...\n",
      "Processing batch 5...\n",
      "Processing batch 6...\n",
      "Processing batch 7...\n",
      "Processing batch 8...\n",
      "Processing batch 9...\n",
      "Processing batch 10...\n",
      "Processing title column...\n",
      "Saved NLP preprocessed chunk to intermediate_data/nlp_chunk_9.pkl\n",
      " - Memory usage: 1259.46 MB\n",
      "Loading intermediate_data/cleaned_chunk_10.pkl...\n",
      "Preprocessing chunk 11 for NLP...\n",
      "Processing text column...\n",
      "Processing batch 1...\n",
      "After processing batch 1 - Memory usage: 963.75 MB\n",
      "Processing batch 2...\n",
      "Processing batch 3...\n",
      "Processing batch 4...\n",
      "Processing batch 5...\n",
      "Processing batch 6...\n",
      "Processing batch 7...\n",
      "Processing batch 8...\n",
      "Processing batch 9...\n",
      "Processing batch 10...\n",
      "Processing title column...\n",
      "Saved NLP preprocessed chunk to intermediate_data/nlp_chunk_10.pkl\n",
      " - Memory usage: 1238.06 MB\n",
      "Loading intermediate_data/cleaned_chunk_11.pkl...\n",
      "Preprocessing chunk 12 for NLP...\n",
      "Processing text column...\n",
      "Processing batch 1...\n",
      "After processing batch 1 - Memory usage: 1114.98 MB\n",
      "Processing batch 2...\n",
      "Processing batch 3...\n",
      "Processing batch 4...\n",
      "Processing batch 5...\n",
      "Processing batch 6...\n",
      "Processing batch 7...\n",
      "Processing batch 8...\n",
      "Processing batch 9...\n",
      "Processing batch 10...\n",
      "Processing title column...\n",
      "Saved NLP preprocessed chunk to intermediate_data/nlp_chunk_11.pkl\n",
      " - Memory usage: 1248.92 MB\n",
      "Loading intermediate_data/cleaned_chunk_12.pkl...\n",
      "Preprocessing chunk 13 for NLP...\n",
      "Processing text column...\n",
      "Processing batch 1...\n",
      "After processing batch 1 - Memory usage: 977.81 MB\n",
      "Processing batch 2...\n",
      "Processing batch 3...\n",
      "Processing batch 4...\n",
      "Processing batch 5...\n",
      "Processing batch 6...\n",
      "Processing batch 7...\n",
      "Processing batch 8...\n",
      "Processing batch 9...\n",
      "Processing batch 10...\n",
      "Processing title column...\n",
      "Saved NLP preprocessed chunk to intermediate_data/nlp_chunk_12.pkl\n",
      " - Memory usage: 1209.46 MB\n",
      "Loading intermediate_data/cleaned_chunk_13.pkl...\n",
      "Preprocessing chunk 14 for NLP...\n",
      "Processing text column...\n",
      "Processing batch 1...\n",
      "After processing batch 1 - Memory usage: 1101.45 MB\n",
      "Processing batch 2...\n",
      "Processing batch 3...\n",
      "Processing batch 4...\n",
      "Processing batch 5...\n",
      "Processing batch 6...\n",
      "Processing batch 7...\n",
      "Processing batch 8...\n",
      "Processing batch 9...\n",
      "Processing batch 10...\n",
      "Processing title column...\n",
      "Saved NLP preprocessed chunk to intermediate_data/nlp_chunk_13.pkl\n",
      " - Memory usage: 1282.06 MB\n",
      "Loading intermediate_data/cleaned_chunk_14.pkl...\n",
      "Preprocessing chunk 15 for NLP...\n",
      "Processing text column...\n",
      "Processing batch 1...\n",
      "After processing batch 1 - Memory usage: 1032.30 MB\n",
      "Processing batch 2...\n",
      "Processing batch 3...\n",
      "Processing batch 4...\n",
      "Processing batch 5...\n",
      "Processing batch 6...\n",
      "Processing batch 7...\n",
      "Processing batch 8...\n",
      "Processing batch 9...\n",
      "Processing batch 10...\n",
      "Processing title column...\n",
      "Saved NLP preprocessed chunk to intermediate_data/nlp_chunk_14.pkl\n",
      " - Memory usage: 1256.21 MB\n",
      "Loading intermediate_data/cleaned_chunk_15.pkl...\n",
      "Preprocessing chunk 16 for NLP...\n",
      "Processing text column...\n",
      "Processing batch 1...\n",
      "After processing batch 1 - Memory usage: 1095.95 MB\n",
      "Processing batch 2...\n",
      "Processing batch 3...\n",
      "Processing batch 4...\n",
      "Processing batch 5...\n",
      "Processing batch 6...\n",
      "Processing batch 7...\n",
      "Processing batch 8...\n",
      "Processing batch 9...\n",
      "Processing batch 10...\n",
      "Processing title column...\n",
      "Saved NLP preprocessed chunk to intermediate_data/nlp_chunk_15.pkl\n",
      " - Memory usage: 1204.93 MB\n",
      "Loading intermediate_data/cleaned_chunk_16.pkl...\n",
      "Preprocessing chunk 17 for NLP...\n",
      "Processing text column...\n",
      "Processing batch 1...\n",
      "After processing batch 1 - Memory usage: 865.61 MB\n",
      "Processing batch 2...\n",
      "Processing batch 3...\n",
      "Processing batch 4...\n",
      "Processing batch 5...\n",
      "Processing batch 6...\n",
      "Processing batch 7...\n",
      "Processing batch 8...\n",
      "Processing batch 9...\n",
      "Processing batch 10...\n",
      "Processing title column...\n",
      "Saved NLP preprocessed chunk to intermediate_data/nlp_chunk_16.pkl\n",
      " - Memory usage: 1212.12 MB\n",
      "Loading intermediate_data/cleaned_chunk_17.pkl...\n",
      "Preprocessing chunk 18 for NLP...\n",
      "Processing text column...\n",
      "Processing batch 1...\n",
      "After processing batch 1 - Memory usage: 887.57 MB\n",
      "Processing batch 2...\n",
      "Processing batch 3...\n",
      "Processing batch 4...\n",
      "Processing batch 5...\n",
      "Processing batch 6...\n",
      "Processing batch 7...\n",
      "Processing batch 8...\n",
      "Processing batch 9...\n",
      "Processing batch 10...\n",
      "Processing title column...\n",
      "Saved NLP preprocessed chunk to intermediate_data/nlp_chunk_17.pkl\n",
      " - Memory usage: 1252.48 MB\n",
      "Loading intermediate_data/cleaned_chunk_18.pkl...\n",
      "Preprocessing chunk 19 for NLP...\n",
      "Processing text column...\n",
      "Processing batch 1...\n",
      "After processing batch 1 - Memory usage: 1077.01 MB\n",
      "Processing batch 2...\n",
      "Processing batch 3...\n",
      "Processing batch 4...\n",
      "Processing batch 5...\n",
      "Processing batch 6...\n",
      "Processing batch 7...\n",
      "Processing batch 8...\n",
      "Processing batch 9...\n",
      "Processing batch 10...\n",
      "Processing title column...\n",
      "Saved NLP preprocessed chunk to intermediate_data/nlp_chunk_18.pkl\n",
      " - Memory usage: 1205.40 MB\n",
      "Loading intermediate_data/cleaned_chunk_19.pkl...\n",
      "Preprocessing chunk 20 for NLP...\n",
      "Processing text column...\n",
      "Processing batch 1...\n",
      "After processing batch 1 - Memory usage: 757.68 MB\n",
      "Processing batch 2...\n",
      "Processing batch 3...\n",
      "Processing title column...\n",
      "Saved NLP preprocessed chunk to intermediate_data/nlp_chunk_19.pkl\n",
      " - Memory usage: 768.32 MB\n",
      "After NLP preprocessing all chunks - Memory usage: 707.32 MB\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text_for_nlp(df):\n",
    "    \"\"\"Apply NLP preprocessing: tokenization, remove stopwords, lemmatization, stemming\"\"\"\n",
    "    df_nlp = df.copy()\n",
    "    \n",
    "    # Initialize tools\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    def process_text(text):\n",
    "        if not isinstance(text, str) or not text:\n",
    "            return [], [], []\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "        \n",
    "        # Lemmatize\n",
    "        lemmatized = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "        \n",
    "        # Stem\n",
    "        stemmed = [stemmer.stem(word) for word in filtered_tokens]\n",
    "        \n",
    "        return filtered_tokens, lemmatized, stemmed\n",
    "    \n",
    "    # Process text and title\n",
    "    if 'clean_text' in df_nlp.columns:\n",
    "        print(\"Processing text column...\")\n",
    "        # Process in batches to avoid memory issues\n",
    "        batch_size = 1000\n",
    "        tokens_list = []\n",
    "        lemmatized_list = []\n",
    "        stemmed_list = []\n",
    "        \n",
    "        for i in range(0, len(df_nlp), batch_size):\n",
    "            print(f\"Processing batch {i//batch_size + 1}...\")\n",
    "            batch = df_nlp['clean_text'].iloc[i:i+batch_size]\n",
    "            \n",
    "            batch_results = [process_text(text) for text in batch]\n",
    "            \n",
    "            # Unpack results\n",
    "            batch_tokens, batch_lemmatized, batch_stemmed = zip(*batch_results)\n",
    "            \n",
    "            tokens_list.extend(batch_tokens)\n",
    "            lemmatized_list.extend(batch_lemmatized)\n",
    "            stemmed_list.extend(batch_stemmed)\n",
    "            \n",
    "            # Print memory usage after each batch\n",
    "            if (i//batch_size) % 10 == 0:\n",
    "                print_memory_usage(f\"After processing batch {i//batch_size + 1}\")\n",
    "        \n",
    "        # Add results to dataframe\n",
    "        df_nlp['tokens'] = tokens_list\n",
    "        df_nlp['lemmatized'] = lemmatized_list\n",
    "        df_nlp['stemmed'] = stemmed_list\n",
    "    \n",
    "    # Process title (simpler as titles are shorter)\n",
    "    if 'clean_title' in df_nlp.columns:\n",
    "        print(\"Processing title column...\")\n",
    "        title_results = [process_text(title) for title in df_nlp['clean_title']]\n",
    "        title_tokens, title_lemmatized, title_stemmed = zip(*title_results)\n",
    "        \n",
    "        df_nlp['title_tokens'] = list(title_tokens)\n",
    "        df_nlp['title_lemmatized'] = list(title_lemmatized)\n",
    "        df_nlp['title_stemmed'] = list(title_stemmed)\n",
    "    \n",
    "    return df_nlp\n",
    "\n",
    "# Process each cleaned chunk for NLP\n",
    "num_chunks = len([f for f in os.listdir('intermediate_data') if f.startswith('cleaned_chunk_')])\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    # Load the cleaned chunk\n",
    "    file_path = f'intermediate_data/cleaned_chunk_{i}.pkl'\n",
    "    print(f\"Loading {file_path}...\")\n",
    "    chunk = pd.read_pickle(file_path)\n",
    "    \n",
    "    # Preprocess for NLP\n",
    "    print(f\"Preprocessing chunk {i+1} for NLP...\")\n",
    "    nlp_chunk = preprocess_text_for_nlp(chunk)\n",
    "    \n",
    "    # Save NLP preprocessed chunk\n",
    "    nlp_file = f'intermediate_data/nlp_chunk_{i}.pkl'\n",
    "    nlp_chunk.to_pickle(nlp_file)\n",
    "    print(f\"Saved NLP preprocessed chunk to {nlp_file}\")\n",
    "    print_memory_usage()\n",
    "    \n",
    "    # Free memory\n",
    "    del chunk, nlp_chunk\n",
    "    gc.collect()\n",
    "\n",
    "print_memory_usage(\"After NLP preprocessing all chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Word Count Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate word count statistics across all chunks\n",
    "def word_count_analysis():\n",
    "    num_chunks = len([f for f in os.listdir('intermediate_data') if f.startswith('nlp_chunk_')])\n",
    "    \n",
    "    # Initialize stats\n",
    "    title_lengths = []\n",
    "    text_lengths = []\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        # Load chunk\n",
    "        file_path = f'intermediate_data/nlp_chunk_{i}.pkl'\n",
    "        print(f\"Analyzing word counts in {file_path}...\")\n",
    "        chunk = pd.read_pickle(file_path)\n",
    "        \n",
    "        # Calculate lengths\n",
    "        if 'tokens' in chunk.columns:\n",
    "            chunk_text_lengths = chunk['tokens'].apply(len)\n",
    "            text_lengths.extend(chunk_text_lengths)\n",
    "        \n",
    "        if 'title_tokens' in chunk.columns:\n",
    "            chunk_title_lengths = chunk['title_tokens'].apply(len)\n",
    "            title_lengths.extend(chunk_title_lengths)\n",
    "        \n",
    "        # Free memory\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Convert to Series for analysis\n",
    "    title_lengths = pd.Series(title_lengths)\n",
    "    text_lengths = pd.Series(text_lengths)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    print(\"\\nTitle word count statistics:\")\n",
    "    print(title_lengths.describe())\n",
    "    \n",
    "    print(\"\\nArticle text word count statistics:\")\n",
    "    print(text_lengths.describe())\n",
    "    \n",
    "    # Create histograms\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(title_lengths, kde=True)\n",
    "    plt.title('Distribution of Title Word Counts')\n",
    "    plt.xlabel('Word Count')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(text_lengths.clip(upper=1000), kde=True)  # Clip to avoid extreme outliers\n",
    "    plt.title('Distribution of Article Word Counts (capped at 1000)')\n",
    "    plt.xlabel('Word Count')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('word_count_distribution.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return title_lengths, text_lengths\n",
    "\n",
    "title_lengths, text_lengths = word_count_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_common_words(field='lemmatized', n=30):\n",
    "    \"\"\"Find most common words across all chunks for a given field\"\"\"\n",
    "    num_chunks = len([f for f in os.listdir('intermediate_data') if f.startswith('nlp_chunk_')])\n",
    "    \n",
    "    # Initialize counter\n",
    "    word_counter = Counter()\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        # Load chunk\n",
    "        file_path = f'intermediate_data/nlp_chunk_{i}.pkl'\n",
    "        print(f\"Finding common words in {file_path}...\")\n",
    "        chunk = pd.read_pickle(file_path)\n",
    "        \n",
    "        # Count words\n",
    "        if field in chunk.columns:\n",
    "            # Flatten list of lists and count\n",
    "            words = [word for word_list in chunk[field] for word in word_list if len(word) > 1]\n",
    "            word_counter.update(words)\n",
    "        \n",
    "        # Free memory\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Get most common words\n",
    "    most_common = word_counter.most_common(n)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    words, counts = zip(*most_common)\n",
    "    sns.barplot(x=list(counts), y=list(words))\n",
    "    plt.title(f'Top {n} Most Common Words')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'most_common_{field}.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(most_common))\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Word Cloud of Top Words')\n",
    "    plt.savefig(f'wordcloud_{field}.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return most_common\n",
    "\n",
    "# Find most common lemmatized words\n",
    "most_common_words = find_most_common_words('lemmatized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Most Common Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_common_tags(n=20):\n",
    "    \"\"\"Find most common tags across all chunks\"\"\"\n",
    "    num_chunks = len([f for f in os.listdir('intermediate_data') if f.startswith('nlp_chunk_')])\n",
    "    \n",
    "    # Initialize counter\n",
    "    tag_counter = Counter()\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        # Load chunk\n",
    "        file_path = f'intermediate_data/nlp_chunk_{i}.pkl'\n",
    "        print(f\"Finding common tags in {file_path}...\")\n",
    "        chunk = pd.read_pickle(file_path)\n",
    "        \n",
    "        # Count tags\n",
    "        if 'tags' in chunk.columns:\n",
    "            # Flatten list of lists and count\n",
    "            tags = [tag for tag_list in chunk['tags'] for tag in tag_list]\n",
    "            tag_counter.update(tags)\n",
    "        \n",
    "        # Free memory\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Get most common tags\n",
    "    most_common = tag_counter.most_common(n)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    tags, counts = zip(*most_common)\n",
    "    sns.barplot(x=list(counts), y=list(tags))\n",
    "    plt.title(f'Top {n} Most Common Tags')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('most_common_tags.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return most_common\n",
    "\n",
    "# Find most common tags\n",
    "most_common_tags = find_most_common_tags()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Authors with Most Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_prolific_authors(n=20):\n",
    "    \"\"\"Find authors with the most articles across all chunks\"\"\"\n",
    "    num_chunks = len([f for f in os.listdir('intermediate_data') if f.startswith('nlp_chunk_')])\n",
    "    \n",
    "    # Initialize counter\n",
    "    author_counter = Counter()\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        # Load chunk\n",
    "        file_path = f'intermediate_data/nlp_chunk_{i}.pkl'\n",
    "        print(f\"Analyzing authors in {file_path}...\")\n",
    "        chunk = pd.read_pickle(file_path)\n",
    "        \n",
    "        # Count authors\n",
    "        if 'authors' in chunk.columns:\n",
    "            # Flatten list of lists and count\n",
    "            authors = [author for author_list in chunk['authors'] for author in author_list]\n",
    "            author_counter.update(authors)\n",
    "        \n",
    "        # Free memory\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Get most prolific authors\n",
    "    most_prolific = author_counter.most_common(n)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    authors, counts = zip(*most_prolific)\n",
    "    sns.barplot(x=list(counts), y=list(authors))\n",
    "    plt.title(f'Top {n} Most Prolific Authors')\n",
    "    plt.xlabel('Number of Articles')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('most_prolific_authors.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return most_prolific\n",
    "\n",
    "# Find most prolific authors\n",
    "most_prolific_authors = find_most_prolific_authors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Most Frequent N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(tokens_list, n=2):\n",
    "    \"\"\"Generate n-grams from a list of tokens\"\"\"\n",
    "    ngrams = []\n",
    "    for tokens in tokens_list:\n",
    "        if len(tokens) >= n:\n",
    "            # Generate n-grams\n",
    "            grams = [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "            ngrams.extend(grams)\n",
    "    return ngrams\n",
    "\n",
    "def find_most_common_ngrams(n_gram=2, top_n=20):\n",
    "    \"\"\"Find most common n-grams across all chunks\"\"\"\n",
    "    num_chunks = len([f for f in os.listdir('intermediate_data') if f.startswith('nlp_chunk_')])\n",
    "    \n",
    "    # Initialize counter\n",
    "    ngram_counter = Counter()\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        # Load chunk\n",
    "        file_path = f'intermediate_data/nlp_chunk_{i}.pkl'\n",
    "        print(f\"Analyzing {n_gram}-grams in {file_path}...\")\n",
    "        chunk = pd.read_pickle(file_path)\n",
    "        \n",
    "        # Generate and count n-grams\n",
    "        if 'lemmatized' in chunk.columns:\n",
    "            chunk_ngrams = generate_ngrams(chunk['lemmatized'], n=n_gram)\n",
    "            ngram_counter.update(chunk_ngrams)\n",
    "        \n",
    "        # Free memory\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Get most common n-grams\n",
    "    most_common = ngram_counter.most_common(top_n)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ngrams, counts = zip(*most_common)\n",
    "    sns.barplot(x=list(counts), y=list(ngrams))\n",
    "    plt.title(f'Top {top_n} Most Common {n_gram}-grams')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'most_common_{n_gram}grams.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return most_common\n",
    "\n",
    "# Find most common bi-grams and tri-grams\n",
    "most_common_bigrams = find_most_common_ngrams(n_gram=2)\n",
    "most_common_trigrams = find_most_common_ngrams(n_gram=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Publication Trends Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_publication_trends():\n",
    "    \"\"\"Analyze publication trends over time\"\"\"\n",
    "    num_chunks = len([f for f in os.listdir('intermediate_data') if f.startswith('nlp_chunk_')])\n",
    "    \n",
    "    # Initialize lists to store timestamps\n",
    "    timestamps = []\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        # Load chunk\n",
    "        file_path = f'intermediate_data/nlp_chunk_{i}.pkl'\n",
    "        print(f\"Analyzing timestamps in {file_path}...\")\n",
    "        chunk = pd.read_pickle(file_path)\n",
    "        \n",
    "        # Extract timestamps\n",
    "        if 'timestamp' in chunk.columns:\n",
    "            timestamps.extend(chunk['timestamp'].dropna())\n",
    "        \n",
    "        # Free memory\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    timestamps_df = pd.DataFrame({'timestamp': timestamps})\n",
    "    \n",
    "    # Extract date components\n",
    "    timestamps_df['year'] = timestamps_df['timestamp'].dt.year\n",
    "    timestamps_df['month'] = timestamps_df['timestamp'].dt.month\n",
    "    timestamps_df['day'] = timestamps_df['timestamp'].dt.day\n",
    "    timestamps_df['hour'] = timestamps_df['timestamp'].dt.hour\n",
    "    timestamps_df['weekday'] = timestamps_df['timestamp'].dt.weekday\n",
    "    \n",
    "    # Create year-month column for trend analysis\n",
    "    timestamps_df['year_month'] = timestamps_df['timestamp'].dt.to_period('M')\n",
    "    \n",
    "    # Monthly publication counts\n",
    "    monthly_counts = timestamps_df['year_month'].value_counts().sort_index()\n",
    "    monthly_counts = monthly_counts.reset_index()\n",
    "    monthly_counts.columns = ['Month', 'Count']\n",
    "    \n",
    "    # Plot monthly trends\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(monthly_counts['Month'].astype(str), monthly_counts['Count'])\n",
    "    plt.title('Monthly Publication Trends')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Number of Articles')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('monthly_publication_trends.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Publication by day of week\n",
    "    weekday_counts = timestamps_df['weekday'].value_counts().sort_index()\n",
    "    weekday_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(weekday_names, weekday_counts)\n",
    "    plt.title('Publication by Day of Week')\n",
    "    plt.xlabel('Day of Week')\n",
    "    plt.ylabel('Number of Articles')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('weekday_publication_trends.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Publication by hour of day\n",
    "    hour_counts = timestamps_df['hour'].value_counts().sort_index()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(hour_counts.index, hour_counts.values)\n",
    "    plt.title('Publication by Hour of Day')\n",
    "    plt.xlabel('Hour (24-hour format)')\n",
    "    plt.ylabel('Number of Articles')\n",
    "    plt.xticks(range(0, 24))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('hourly_publication_trends.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return timestamps_df\n",
    "\n",
    "# Analyze publication trends\n",
    "time_analysis = analyze_publication_trends()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section requires additional memory and processing power\n",
    "# Uncomment and run if your system can handle it\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def perform_topic_modeling(n_topics=10, n_top_words=15):\n",
    "    \"\"\"Perform topic modeling using LDA\"\"\"\n",
    "    # This requires loading all data into memory\n",
    "    # Consider using a subset if memory is limited\n",
    "    \n",
    "    # Concatenate all lemmatized tokens into documents\n",
    "    documents = []\n",
    "    \n",
    "    num_chunks = len([f for f in os.listdir('intermediate_data') if f.startswith('nlp_chunk_')])\n",
    "    \n",
    "    # Use a subset if data is too large\n",
    "    max_docs = 10000  # Adjust based on memory constraints\n",
    "    doc_count = 0\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        if doc_count >= max_docs:\n",
    "            break\n",
    "            \n",
    "        # Load chunk\n",
    "        file_path = f'intermediate_data/nlp_chunk_{i}.pkl'\n",
    "        print(f\"Loading {file_path} for topic modeling...\")\n",
    "        chunk = pd.read_pickle(file_path)\n",
    "        \n",
    "        # Convert lemmatized tokens to documents\n",
    "        if 'lemmatized' in chunk.columns:\n",
    "            chunk_docs = [' '.join(tokens) for tokens in chunk['lemmatized']]\n",
    "            documents.extend(chunk_docs[:max_docs-doc_count])\n",
    "            doc_count += len(chunk_docs[:max_docs-doc_count])\n",
    "        \n",
    "        # Free memory\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"Performing topic modeling on {len(documents)} documents...\")\n",
    "    \n",
    "    # Create document-term matrix\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=10000)\n",
    "    dtm = vectorizer.fit_transform(documents)\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Fit LDA model\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics,\n",
    "        random_state=42,\n",
    "        max_iter=5,  # Reduce for memory constraints\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    lda.fit(dtm)\n",
    "    \n",
    "    # Print topics\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        print(f\"Topic #{topic_idx+1}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words-1:-1]]))\n",
    "        print()\n",
    "    \n",
    "    return lda, vectorizer, feature_names\n",
    "\n",
    "# Perform topic modeling\n",
    "lda_model, vectorizer, feature_names = perform_topic_modeling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save analysis results\n",
    "results = {\n",
    "    'most_common_words': most_common_words,\n",
    "    'most_common_tags': most_common_tags,\n",
    "    'most_prolific_authors': most_prolific_authors,\n",
    "    'most_common_bigrams': most_common_bigrams,\n",
    "    'most_common_trigrams': most_common_trigrams\n",
    "}\n",
    "\n",
    "# Save results as pickle\n",
    "with open('analysis_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "print(\"Analysis results saved to 'analysis_results.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NLP Analysis of Medium Articles - Summary\")\n",
    "print(\"==========================================\\n\")\n",
    "\n",
    "# Load results\n",
    "with open('analysis_results.pkl', 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "print(\"Top 10 Most Common Words:\")\n",
    "for word, count in results['most_common_words'][:10]:\n",
    "    print(f\"  {word}: {count}\")\n",
    "print()\n",
    "\n",
    "print(\"Top 10 Most Common Tags:\")\n",
    "for tag, count in results['most_common_tags'][:10]:\n",
    "    print(f\"  {tag}: {count}\")\n",
    "print()\n",
    "\n",
    "print(\"Top 10 Most Prolific Authors:\")\n",
    "for author, count in results['most_prolific_authors'][:10]:\n",
    "    print(f\"  {author}: {count} articles\")\n",
    "print()\n",
    "\n",
    "print(\"Top 10 Most Common Bigrams:\")\n",
    "for bigram, count in results['most_common_bigrams'][:10]:\n",
    "    print(f\"  {bigram}: {count}\")\n",
    "print()\n",
    "\n",
    "print(\"Top 10 Most Common Trigrams:\")\n",
    "for trigram, count in results['most_common_trigrams'][:10]:\n",
    "    print(f\"  {trigram}: {count}\")\n",
    "print()\n",
    "\n",
    "print(\"Analysis completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 NLP chunks to merge.\n",
      "Loading intermediate_data\\nlp_chunk_0.pkl...\n",
      "Loading intermediate_data\\nlp_chunk_1.pkl...\n",
      "Loading intermediate_data\\nlp_chunk_10.pkl...\n",
      "Loading intermediate_data\\nlp_chunk_11.pkl...\n",
      "Loading intermediate_data\\nlp_chunk_12.pkl...\n",
      "Loading intermediate_data\\nlp_chunk_13.pkl...\n",
      "Loading intermediate_data\\nlp_chunk_14.pkl...\n",
      "Loading intermediate_data\\nlp_chunk_15.pkl...\n",
      "Loading intermediate_data\\nlp_chunk_16.pkl...\n",
      "Loading intermediate_data\\nlp_chunk_17.pkl...\n",
      "Loading intermediate_data\\nlp_chunk_18.pkl...\n",
      "Loading intermediate_data\\nlp_chunk_19.pkl...\n",
      "Loading intermediate_data\\nlp_chunk_2.pkl...\n",
      "Loading intermediate_data\\nlp_chunk_3.pkl...\n",
      "Loading intermediate_data\\nlp_chunk_4.pkl...\n",
      "Loading intermediate_data\\nlp_chunk_5.pkl...\n",
      "Loading intermediate_data\\nlp_chunk_6.pkl...\n",
      "Loading intermediate_data\\nlp_chunk_7.pkl...\n",
      "Loading intermediate_data\\nlp_chunk_8.pkl...\n",
      "Loading intermediate_data\\nlp_chunk_9.pkl...\n",
      "Final merged DataFrame shape: (192363, 8)\n",
      "Saved final merged data to 'final_nlp_data.pkl'.\n"
     ]
    }
   ],
   "source": [
    "# Directory where your intermediate chunks are stored\n",
    "import os\n",
    "intermediate_folder = 'intermediate_data'\n",
    "\n",
    "# Identify all NLP processed chunk files\n",
    "chunk_files = sorted([\n",
    "    f for f in os.listdir(intermediate_folder)\n",
    "    if f.startswith('nlp_chunk_') and f.endswith('.pkl')\n",
    "])\n",
    "\n",
    "print(f\"Found {len(chunk_files)} NLP chunks to merge.\")\n",
    "\n",
    "# Initialize list to collect DataFrames\n",
    "all_chunks = []\n",
    "\n",
    "# Load and append each chunk\n",
    "for i, file in enumerate(chunk_files):\n",
    "    file_path = os.path.join(intermediate_folder, file)\n",
    "    print(f\"Loading {file_path}...\")\n",
    "\n",
    "    chunk_df = pd.read_pickle(file_path)\n",
    "    chunk_df.drop(columns=['title', 'text', 'tokens', 'title_tokens', 'title_lemmatized','lemmatized'], inplace=True)\n",
    "    all_chunks.append(chunk_df)\n",
    "\n",
    "    # Clear memory from last chunk\n",
    "    del chunk_df\n",
    "    gc.collect()\n",
    "\n",
    "# Concatenate all chunks\n",
    "final_df = pd.concat(all_chunks, ignore_index=True)\n",
    "print(f\"Final merged DataFrame shape: {final_df.shape}\")\n",
    "\n",
    "# Optional: drop duplicate articles if needed\n",
    "# final_df.drop_duplicates(subset=['title', 'url'], inplace=True)\n",
    "\n",
    "# Save the final dataset as a pickle (fast & preserves Python objects)\n",
    "final_df.to_pickle('final_nlp_data.pkl')\n",
    "print(\"Saved final merged data to 'final_nlp_data.pkl'.\")\n",
    "\n",
    "# Optionally also save as CSV (if you want to inspect or use outside Python)\n",
    "final_df.to_csv('final_nlp_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clean up intermediate files\n",
    "'''\n",
    "import shutil\n",
    "\n",
    "def cleanup_intermediate_files():\n",
    "    \"\"\"Remove intermediate files to free up disk space\"\"\"\n",
    "    print(\"Cleaning up intermediate files...\")\n",
    "    if os.path.exists('intermediate_data'):\n",
    "        shutil.rmtree('intermediate_data')\n",
    "        print(\"Intermediate files removed.\")\n",
    "\n",
    "# cleanup_intermediate_files()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
