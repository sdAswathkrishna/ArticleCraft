{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medium Articles NLP Analysis\n",
    "\n",
    "This notebook performs Natural Language Processing on a large dataset of Medium articles. Due to the size of the dataset, we'll implement techniques to handle memory constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "# NLP libraries\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# For memory usage tracking\n",
    "import psutil\n",
    "\n",
    "# # Download necessary NLTK data\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "def print_memory_usage(message=\"\"):\n",
    "    \"\"\"Print memory usage with optional message\"\"\"\n",
    "    memory_mb = get_memory_usage()\n",
    "    print(f\"{message} - Memory usage: {memory_mb:.2f} MB\")\n",
    "    \n",
    "# Create directory for intermediate results if it doesn't exist\n",
    "if not os.path.exists('intermediate_data'):\n",
    "    os.makedirs('intermediate_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use chunking to handle large files\n",
    "\n",
    "def load_dataset_in_chunks(file_path, chunk_size=10000):\n",
    "    \"\"\"Load large Csv dataset in chunks\"\"\"\n",
    "    print(f\"Loading dataset from {file_path}...\")\n",
    "    \n",
    "    chunks = []\n",
    "    total_rows = 0\n",
    "    \n",
    "    # Use chunksize to load in batches\n",
    "    for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):\n",
    "        print(f\"Loaded chunk {i+1} with {len(chunk)} rows\")\n",
    "        chunks.append(chunk)\n",
    "        total_rows += len(chunk)\n",
    "        print_memory_usage()\n",
    "    \n",
    "    print(f\"Total rows loaded: {total_rows}\")\n",
    "    return chunks\n",
    "\n",
    "# Replace 'medium_articles.json' with your actual file path\n",
    "file_path = 'medium_articles.csv'  \n",
    "data_chunks = load_dataset_in_chunks(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the first chunk to understand the data structure\n",
    "sample_df = data_chunks[0]\n",
    "\n",
    "# Display basic information\n",
    "print(\"Sample data structure:\")\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataframe info\n",
    "print(\"\\nDataframe info:\")\n",
    "sample_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display statistics for numeric columns (if any)\n",
    "print(\"\\nDescriptive statistics:\")\n",
    "sample_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for column data types and null values across all chunks\n",
    "def analyze_all_chunks(chunks):\n",
    "    total_rows = 0\n",
    "    null_counts = {}\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        total_rows += len(chunk)\n",
    "        \n",
    "        # Count nulls in this chunk\n",
    "        chunk_nulls = chunk.isnull().sum()\n",
    "        \n",
    "        # Update total null counts\n",
    "        for col in chunk.columns:\n",
    "            if col not in null_counts:\n",
    "                null_counts[col] = 0\n",
    "            null_counts[col] += chunk_nulls[col]\n",
    "    \n",
    "    print(f\"Total rows across all chunks: {total_rows}\")\n",
    "    print(\"\\nNull values per column:\")\n",
    "    for col, count in null_counts.items():\n",
    "        percentage = (count / total_rows) * 100\n",
    "        print(f\"{col}: {count} nulls ({percentage:.2f}%)\")\n",
    "\n",
    "analyze_all_chunks(data_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast  # Add this import at the top\n",
    "\n",
    "def convert_data_types(df):\n",
    "    \"\"\"Convert data types for timestamp, authors, and tags\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    if 'timestamp' in df_copy.columns:\n",
    "        df_copy['timestamp'] = pd.to_datetime(df_copy['timestamp'], errors='coerce')\n",
    "\n",
    "    # Helper function to safely evaluate strings into lists\n",
    "    def safe_parse_list(x):\n",
    "        if isinstance(x, str):\n",
    "            try:\n",
    "                return ast.literal_eval(x)\n",
    "            except (ValueError, SyntaxError):\n",
    "                return []\n",
    "        elif isinstance(x, list):\n",
    "            return x\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    # Ensure authors and tags are proper lists\n",
    "    if 'authors' in df_copy.columns:\n",
    "        df_copy['authors'] = df_copy['authors'].apply(safe_parse_list)\n",
    "\n",
    "    if 'tags' in df_copy.columns:\n",
    "        df_copy['tags'] = df_copy['tags'].apply(safe_parse_list)\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Process each chunk and save as intermediate file\n",
    "for i, chunk in enumerate(data_chunks):\n",
    "    print(f\"Converting data types for chunk {i+1}...\")\n",
    "    converted_chunk = convert_data_types(chunk)\n",
    "    \n",
    "    # Save intermediate result\n",
    "    intermediate_file = f'intermediate_data/converted_chunk_{i}.pkl'\n",
    "    converted_chunk.to_pickle(intermediate_file)\n",
    "    print(f\"Saved converted chunk to {intermediate_file}\")\n",
    "    print_memory_usage()\n",
    "\n",
    "# Clear memory\n",
    "del data_chunks\n",
    "gc.collect()\n",
    "print_memory_usage(\"After clearing data chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"Clean data by removing nulls, filling empty lists, and cleaning text\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Remove rows with null values in critical columns\n",
    "    critical_cols = ['title', 'text', 'url']\n",
    "    df_clean = df_clean.dropna(subset=critical_cols)\n",
    "    \n",
    "    # Replace empty lists\n",
    "    if 'authors' in df_clean.columns:\n",
    "        df_clean['authors'] = df_clean['authors'].apply(lambda x: ['Unknown'] if not x else x)\n",
    "    \n",
    "    if 'tags' in df_clean.columns:\n",
    "        df_clean['tags'] = df_clean['tags'].apply(lambda x: ['Untagged'] if not x else x)\n",
    "    \n",
    "    # Clean text columns\n",
    "    def clean_text(text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        \n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()    \n",
    "        \n",
    "        # Remove newline characters\n",
    "        text = re.sub(r'\\n+', ' ', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    # Apply text cleaning to title and text columns\n",
    "    if 'title' in df_clean.columns:\n",
    "        df_clean['clean_title'] = df_clean['title'].apply(clean_text)\n",
    "    \n",
    "    if 'text' in df_clean.columns:\n",
    "        df_clean['clean_text'] = df_clean['text'].apply(clean_text)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Process each converted chunk and save as intermediate file\n",
    "num_chunks = len([f for f in os.listdir('intermediate_data') if f.startswith('converted_chunk_')])\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    # Load the converted chunk\n",
    "    file_path = f'intermediate_data/converted_chunk_{i}.pkl'\n",
    "    print(f\"Loading {file_path}...\")\n",
    "    chunk = pd.read_pickle(file_path)\n",
    "    \n",
    "    # Clean the chunk\n",
    "    print(f\"Cleaning chunk {i+1}...\")\n",
    "    cleaned_chunk = clean_data(chunk)\n",
    "    \n",
    "    # Save cleaned chunk\n",
    "    cleaned_file = f'intermediate_data/cleaned_chunk_{i}.pkl'\n",
    "    cleaned_chunk.to_pickle(cleaned_file)\n",
    "    print(f\"Saved cleaned chunk to {cleaned_file}\")\n",
    "    print_memory_usage()\n",
    "    \n",
    "    # Free memory\n",
    "    del chunk, cleaned_chunk\n",
    "    gc.collect()\n",
    "\n",
    "print_memory_usage(\"After cleaning all chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. NLP Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_for_nlp(df):\n",
    "    \"\"\"Apply NLP preprocessing: tokenization, remove stopwords, lemmatization, stemming\"\"\"\n",
    "    df_nlp = df.copy()\n",
    "    \n",
    "    # Initialize tools\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    def process_text(text):\n",
    "        if not isinstance(text, str) or not text:\n",
    "            return [], [], []\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "        \n",
    "        # Lemmatize\n",
    "        lemmatized = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "        \n",
    "        # Stem\n",
    "        stemmed = [stemmer.stem(word) for word in filtered_tokens]\n",
    "        \n",
    "        return filtered_tokens, lemmatized, stemmed\n",
    "    \n",
    "    # Process text and title\n",
    "    if 'clean_text' in df_nlp.columns:\n",
    "        print(\"Processing text column...\")\n",
    "        # Process in batches to avoid memory issues\n",
    "        batch_size = 1000\n",
    "        tokens_list = []\n",
    "        lemmatized_list = []\n",
    "        stemmed_list = []\n",
    "        \n",
    "        for i in range(0, len(df_nlp), batch_size):\n",
    "            print(f\"Processing batch {i//batch_size + 1}...\")\n",
    "            batch = df_nlp['clean_text'].iloc[i:i+batch_size]\n",
    "            \n",
    "            batch_results = [process_text(text) for text in batch]\n",
    "            \n",
    "            # Unpack results\n",
    "            batch_tokens, batch_lemmatized, batch_stemmed = zip(*batch_results)\n",
    "            \n",
    "            tokens_list.extend(batch_tokens)\n",
    "            lemmatized_list.extend(batch_lemmatized)\n",
    "            stemmed_list.extend(batch_stemmed)\n",
    "            \n",
    "            # Print memory usage after each batch\n",
    "            if (i//batch_size) % 10 == 0:\n",
    "                print_memory_usage(f\"After processing batch {i//batch_size + 1}\")\n",
    "        \n",
    "        # Add results to dataframe\n",
    "        df_nlp['tokens'] = tokens_list\n",
    "        df_nlp['lemmatized'] = lemmatized_list\n",
    "        df_nlp['stemmed'] = stemmed_list\n",
    "    \n",
    "    # Process title (simpler as titles are shorter)\n",
    "    if 'clean_title' in df_nlp.columns:\n",
    "        print(\"Processing title column...\")\n",
    "        title_results = [process_text(title) for title in df_nlp['clean_title']]\n",
    "        title_tokens, title_lemmatized, title_stemmed = zip(*title_results)\n",
    "        \n",
    "        df_nlp['title_tokens'] = list(title_tokens)\n",
    "        df_nlp['title_lemmatized'] = list(title_lemmatized)\n",
    "        df_nlp['title_stemmed'] = list(title_stemmed)\n",
    "    \n",
    "    return df_nlp\n",
    "\n",
    "# Process each cleaned chunk for NLP\n",
    "num_chunks = len([f for f in os.listdir('intermediate_data') if f.startswith('cleaned_chunk_')])\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    # Load the cleaned chunk\n",
    "    file_path = f'intermediate_data/cleaned_chunk_{i}.pkl'\n",
    "    print(f\"Loading {file_path}...\")\n",
    "    chunk = pd.read_pickle(file_path)\n",
    "    \n",
    "    # Preprocess for NLP\n",
    "    print(f\"Preprocessing chunk {i+1} for NLP...\")\n",
    "    nlp_chunk = preprocess_text_for_nlp(chunk)\n",
    "    \n",
    "    # Save NLP preprocessed chunk\n",
    "    nlp_file = f'intermediate_data/nlp_chunk_{i}.pkl'\n",
    "    nlp_chunk.to_pickle(nlp_file)\n",
    "    print(f\"Saved NLP preprocessed chunk to {nlp_file}\")\n",
    "    print_memory_usage()\n",
    "    \n",
    "    # Free memory\n",
    "    del chunk, nlp_chunk\n",
    "    gc.collect()\n",
    "\n",
    "print_memory_usage(\"After NLP preprocessing all chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Word Count Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate word count statistics across all chunks\n",
    "def word_count_analysis():\n",
    "    num_chunks = len([f for f in os.listdir('intermediate_data') if f.startswith('nlp_chunk_')])\n",
    "    \n",
    "    # Initialize stats\n",
    "    title_lengths = []\n",
    "    text_lengths = []\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        # Load chunk\n",
    "        file_path = f'intermediate_data/nlp_chunk_{i}.pkl'\n",
    "        print(f\"Analyzing word counts in {file_path}...\")\n",
    "        chunk = pd.read_pickle(file_path)\n",
    "        \n",
    "        # Calculate lengths\n",
    "        if 'tokens' in chunk.columns:\n",
    "            chunk_text_lengths = chunk['tokens'].apply(len)\n",
    "            text_lengths.extend(chunk_text_lengths)\n",
    "        \n",
    "        if 'title_tokens' in chunk.columns:\n",
    "            chunk_title_lengths = chunk['title_tokens'].apply(len)\n",
    "            title_lengths.extend(chunk_title_lengths)\n",
    "        \n",
    "        # Free memory\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Convert to Series for analysis\n",
    "    title_lengths = pd.Series(title_lengths)\n",
    "    text_lengths = pd.Series(text_lengths)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    print(\"\\nTitle word count statistics:\")\n",
    "    print(title_lengths.describe())\n",
    "    \n",
    "    print(\"\\nArticle text word count statistics:\")\n",
    "    print(text_lengths.describe())\n",
    "    \n",
    "    # Create histograms\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(title_lengths, kde=True)\n",
    "    plt.title('Distribution of Title Word Counts')\n",
    "    plt.xlabel('Word Count')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(text_lengths.clip(upper=1000), kde=True)  # Clip to avoid extreme outliers\n",
    "    plt.title('Distribution of Article Word Counts (capped at 1000)')\n",
    "    plt.xlabel('Word Count')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('word_count_distribution.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return title_lengths, text_lengths\n",
    "\n",
    "title_lengths, text_lengths = word_count_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_common_words(field='lemmatized', n=30):\n",
    "    \"\"\"Find most common words across all chunks for a given field\"\"\"\n",
    "    num_chunks = len([f for f in os.listdir('intermediate_data') if f.startswith('nlp_chunk_')])\n",
    "    \n",
    "    # Initialize counter\n",
    "    word_counter = Counter()\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        # Load chunk\n",
    "        file_path = f'intermediate_data/nlp_chunk_{i}.pkl'\n",
    "        print(f\"Finding common words in {file_path}...\")\n",
    "        chunk = pd.read_pickle(file_path)\n",
    "        \n",
    "        # Count words\n",
    "        if field in chunk.columns:\n",
    "            # Flatten list of lists and count\n",
    "            words = [word for word_list in chunk[field] for word in word_list if len(word) > 1]\n",
    "            word_counter.update(words)\n",
    "        \n",
    "        # Free memory\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Get most common words\n",
    "    most_common = word_counter.most_common(n)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    words, counts = zip(*most_common)\n",
    "    sns.barplot(x=list(counts), y=list(words))\n",
    "    plt.title(f'Top {n} Most Common Words')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'most_common_{field}.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(most_common))\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Word Cloud of Top Words')\n",
    "    plt.savefig(f'wordcloud_{field}.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return most_common\n",
    "\n",
    "# Find most common lemmatized words\n",
    "most_common_words = find_most_common_words('lemmatized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Most Common Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_common_tags(n=20):\n",
    "    \"\"\"Find most common tags across all chunks\"\"\"\n",
    "    num_chunks = len([f for f in os.listdir('intermediate_data') if f.startswith('nlp_chunk_')])\n",
    "    \n",
    "    # Initialize counter\n",
    "    tag_counter = Counter()\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        # Load chunk\n",
    "        file_path = f'intermediate_data/nlp_chunk_{i}.pkl'\n",
    "        print(f\"Finding common tags in {file_path}...\")\n",
    "        chunk = pd.read_pickle(file_path)\n",
    "        \n",
    "        # Count tags\n",
    "        if 'tags' in chunk.columns:\n",
    "            # Flatten list of lists and count\n",
    "            tags = [tag for tag_list in chunk['tags'] for tag in tag_list]\n",
    "            tag_counter.update(tags)\n",
    "        \n",
    "        # Free memory\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Get most common tags\n",
    "    most_common = tag_counter.most_common(n)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    tags, counts = zip(*most_common)\n",
    "    sns.barplot(x=list(counts), y=list(tags))\n",
    "    plt.title(f'Top {n} Most Common Tags')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('most_common_tags.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return most_common\n",
    "\n",
    "# Find most common tags\n",
    "most_common_tags = find_most_common_tags()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Authors with Most Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_prolific_authors(n=20):\n",
    "    \"\"\"Find authors with the most articles across all chunks\"\"\"\n",
    "    num_chunks = len([f for f in os.listdir('intermediate_data') if f.startswith('nlp_chunk_')])\n",
    "    \n",
    "    # Initialize counter\n",
    "    author_counter = Counter()\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        # Load chunk\n",
    "        file_path = f'intermediate_data/nlp_chunk_{i}.pkl'\n",
    "        print(f\"Analyzing authors in {file_path}...\")\n",
    "        chunk = pd.read_pickle(file_path)\n",
    "        \n",
    "        # Count authors\n",
    "        if 'authors' in chunk.columns:\n",
    "            # Flatten list of lists and count\n",
    "            authors = [author for author_list in chunk['authors'] for author in author_list]\n",
    "            author_counter.update(authors)\n",
    "        \n",
    "        # Free memory\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Get most prolific authors\n",
    "    most_prolific = author_counter.most_common(n)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    authors, counts = zip(*most_prolific)\n",
    "    sns.barplot(x=list(counts), y=list(authors))\n",
    "    plt.title(f'Top {n} Most Prolific Authors')\n",
    "    plt.xlabel('Number of Articles')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('most_prolific_authors.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return most_prolific\n",
    "\n",
    "# Find most prolific authors\n",
    "most_prolific_authors = find_most_prolific_authors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Most Frequent N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(tokens_list, n=2):\n",
    "    \"\"\"Generate n-grams from a list of tokens\"\"\"\n",
    "    ngrams = []\n",
    "    for tokens in tokens_list:\n",
    "        if len(tokens) >= n:\n",
    "            # Generate n-grams\n",
    "            grams = [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "            ngrams.extend(grams)\n",
    "    return ngrams\n",
    "\n",
    "def find_most_common_ngrams(n_gram=2, top_n=20):\n",
    "    \"\"\"Find most common n-grams across all chunks\"\"\"\n",
    "    num_chunks = len([f for f in os.listdir('intermediate_data') if f.startswith('nlp_chunk_')])\n",
    "    \n",
    "    # Initialize counter\n",
    "    ngram_counter = Counter()\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        # Load chunk\n",
    "        file_path = f'intermediate_data/nlp_chunk_{i}.pkl'\n",
    "        print(f\"Analyzing {n_gram}-grams in {file_path}...\")\n",
    "        chunk = pd.read_pickle(file_path)\n",
    "        \n",
    "        # Generate and count n-grams\n",
    "        if 'lemmatized' in chunk.columns:\n",
    "            chunk_ngrams = generate_ngrams(chunk['lemmatized'], n=n_gram)\n",
    "            ngram_counter.update(chunk_ngrams)\n",
    "        \n",
    "        # Free memory\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Get most common n-grams\n",
    "    most_common = ngram_counter.most_common(top_n)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ngrams, counts = zip(*most_common)\n",
    "    sns.barplot(x=list(counts), y=list(ngrams))\n",
    "    plt.title(f'Top {top_n} Most Common {n_gram}-grams')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'most_common_{n_gram}grams.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return most_common\n",
    "\n",
    "# Find most common bi-grams and tri-grams\n",
    "most_common_bigrams = find_most_common_ngrams(n_gram=2)\n",
    "most_common_trigrams = find_most_common_ngrams(n_gram=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Publication Trends Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_publication_trends():\n",
    "    \"\"\"Analyze publication trends over time\"\"\"\n",
    "    num_chunks = len([f for f in os.listdir('intermediate_data') if f.startswith('nlp_chunk_')])\n",
    "    \n",
    "    # Initialize lists to store timestamps\n",
    "    timestamps = []\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        # Load chunk\n",
    "        file_path = f'intermediate_data/nlp_chunk_{i}.pkl'\n",
    "        print(f\"Analyzing timestamps in {file_path}...\")\n",
    "        chunk = pd.read_pickle(file_path)\n",
    "        \n",
    "        # Extract timestamps\n",
    "        if 'timestamp' in chunk.columns:\n",
    "            timestamps.extend(chunk['timestamp'].dropna())\n",
    "        \n",
    "        # Free memory\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    timestamps_df = pd.DataFrame({'timestamp': timestamps})\n",
    "    \n",
    "    # Extract date components\n",
    "    timestamps_df['year'] = timestamps_df['timestamp'].dt.year\n",
    "    timestamps_df['month'] = timestamps_df['timestamp'].dt.month\n",
    "    timestamps_df['day'] = timestamps_df['timestamp'].dt.day\n",
    "    timestamps_df['hour'] = timestamps_df['timestamp'].dt.hour\n",
    "    timestamps_df['weekday'] = timestamps_df['timestamp'].dt.weekday\n",
    "    \n",
    "    # Create year-month column for trend analysis\n",
    "    timestamps_df['year_month'] = timestamps_df['timestamp'].dt.to_period('M')\n",
    "    \n",
    "    # Monthly publication counts\n",
    "    monthly_counts = timestamps_df['year_month'].value_counts().sort_index()\n",
    "    monthly_counts = monthly_counts.reset_index()\n",
    "    monthly_counts.columns = ['Month', 'Count']\n",
    "    \n",
    "    # Plot monthly trends\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(monthly_counts['Month'].astype(str), monthly_counts['Count'])\n",
    "    plt.title('Monthly Publication Trends')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Number of Articles')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('monthly_publication_trends.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Publication by day of week\n",
    "    weekday_counts = timestamps_df['weekday'].value_counts().sort_index()\n",
    "    weekday_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(weekday_names, weekday_counts)\n",
    "    plt.title('Publication by Day of Week')\n",
    "    plt.xlabel('Day of Week')\n",
    "    plt.ylabel('Number of Articles')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('weekday_publication_trends.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Publication by hour of day\n",
    "    hour_counts = timestamps_df['hour'].value_counts().sort_index()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(hour_counts.index, hour_counts.values)\n",
    "    plt.title('Publication by Hour of Day')\n",
    "    plt.xlabel('Hour (24-hour format)')\n",
    "    plt.ylabel('Number of Articles')\n",
    "    plt.xticks(range(0, 24))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('hourly_publication_trends.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return timestamps_df\n",
    "\n",
    "# Analyze publication trends\n",
    "time_analysis = analyze_publication_trends()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section requires additional memory and processing power\n",
    "# Uncomment and run if your system can handle it\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def perform_topic_modeling(n_topics=10, n_top_words=15):\n",
    "    \"\"\"Perform topic modeling using LDA\"\"\"\n",
    "    # This requires loading all data into memory\n",
    "    # Consider using a subset if memory is limited\n",
    "    \n",
    "    # Concatenate all lemmatized tokens into documents\n",
    "    documents = []\n",
    "    \n",
    "    num_chunks = len([f for f in os.listdir('intermediate_data') if f.startswith('nlp_chunk_')])\n",
    "    \n",
    "    # Use a subset if data is too large\n",
    "    max_docs = 10000  # Adjust based on memory constraints\n",
    "    doc_count = 0\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        if doc_count >= max_docs:\n",
    "            break\n",
    "            \n",
    "        # Load chunk\n",
    "        file_path = f'intermediate_data/nlp_chunk_{i}.pkl'\n",
    "        print(f\"Loading {file_path} for topic modeling...\")\n",
    "        chunk = pd.read_pickle(file_path)\n",
    "        \n",
    "        # Convert lemmatized tokens to documents\n",
    "        if 'lemmatized' in chunk.columns:\n",
    "            chunk_docs = [' '.join(tokens) for tokens in chunk['lemmatized']]\n",
    "            documents.extend(chunk_docs[:max_docs-doc_count])\n",
    "            doc_count += len(chunk_docs[:max_docs-doc_count])\n",
    "        \n",
    "        # Free memory\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"Performing topic modeling on {len(documents)} documents...\")\n",
    "    \n",
    "    # Create document-term matrix\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=10000)\n",
    "    dtm = vectorizer.fit_transform(documents)\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Fit LDA model\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics,\n",
    "        random_state=42,\n",
    "        max_iter=5,  # Reduce for memory constraints\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    lda.fit(dtm)\n",
    "    \n",
    "    # Print topics\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        print(f\"Topic #{topic_idx+1}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words-1:-1]]))\n",
    "        print()\n",
    "    \n",
    "    return lda, vectorizer, feature_names\n",
    "\n",
    "# Perform topic modeling\n",
    "lda_model, vectorizer, feature_names = perform_topic_modeling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save analysis results\n",
    "results = {\n",
    "    'most_common_words': most_common_words,\n",
    "    'most_common_tags': most_common_tags,\n",
    "    'most_prolific_authors': most_prolific_authors,\n",
    "    'most_common_bigrams': most_common_bigrams,\n",
    "    'most_common_trigrams': most_common_trigrams\n",
    "}\n",
    "\n",
    "# Save results as pickle\n",
    "with open('analysis_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "print(\"Analysis results saved to 'analysis_results.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NLP Analysis of Medium Articles - Summary\")\n",
    "print(\"==========================================\\n\")\n",
    "\n",
    "# Load results\n",
    "with open('analysis_results.pkl', 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "print(\"Top 10 Most Common Words:\")\n",
    "for word, count in results['most_common_words'][:10]:\n",
    "    print(f\"  {word}: {count}\")\n",
    "print()\n",
    "\n",
    "print(\"Top 10 Most Common Tags:\")\n",
    "for tag, count in results['most_common_tags'][:10]:\n",
    "    print(f\"  {tag}: {count}\")\n",
    "print()\n",
    "\n",
    "print(\"Top 10 Most Prolific Authors:\")\n",
    "for author, count in results['most_prolific_authors'][:10]:\n",
    "    print(f\"  {author}: {count} articles\")\n",
    "print()\n",
    "\n",
    "print(\"Top 10 Most Common Bigrams:\")\n",
    "for bigram, count in results['most_common_bigrams'][:10]:\n",
    "    print(f\"  {bigram}: {count}\")\n",
    "print()\n",
    "\n",
    "print(\"Top 10 Most Common Trigrams:\")\n",
    "for trigram, count in results['most_common_trigrams'][:10]:\n",
    "    print(f\"  {trigram}: {count}\")\n",
    "print()\n",
    "\n",
    "print(\"Analysis completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where your intermediate chunks are stored\n",
    "intermediate_folder = 'intermediate_data'\n",
    "\n",
    "# Identify all NLP processed chunk files\n",
    "chunk_files = sorted([\n",
    "    f for f in os.listdir(intermediate_folder)\n",
    "    if f.startswith('nlp_chunk_') and f.endswith('.pkl')\n",
    "])\n",
    "\n",
    "print(f\"Found {len(chunk_files)} NLP chunks to merge.\")\n",
    "\n",
    "# Initialize list to collect DataFrames\n",
    "all_chunks = []\n",
    "\n",
    "# Load and append each chunk\n",
    "for i, file in enumerate(chunk_files):\n",
    "    file_path = os.path.join(intermediate_folder, file)\n",
    "    print(f\"Loading {file_path}...\")\n",
    "    \n",
    "    chunk_df = pd.read_pickle(file_path)\n",
    "    all_chunks.append(chunk_df)\n",
    "    \n",
    "    # Clear memory from last chunk\n",
    "    del chunk_df\n",
    "    gc.collect()\n",
    "\n",
    "# Concatenate all chunks\n",
    "final_df = pd.concat(all_chunks, ignore_index=True)\n",
    "print(f\"Final merged DataFrame shape: {final_df.shape}\")\n",
    "\n",
    "# Optional: drop duplicate articles if needed\n",
    "# final_df.drop_duplicates(subset=['title', 'url'], inplace=True)\n",
    "\n",
    "# Save the final dataset as a pickle (fast & preserves Python objects)\n",
    "final_df.to_pickle('final_nlp_data.pkl')\n",
    "print(\"Saved final merged data to 'final_nlp_data.pkl'.\")\n",
    "\n",
    "# Optionally also save as CSV (if you want to inspect or use outside Python)\n",
    "final_df.to_csv('final_nlp_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clean up intermediate files\n",
    "'''\n",
    "import shutil\n",
    "\n",
    "def cleanup_intermediate_files():\n",
    "    \"\"\"Remove intermediate files to free up disk space\"\"\"\n",
    "    print(\"Cleaning up intermediate files...\")\n",
    "    if os.path.exists('intermediate_data'):\n",
    "        shutil.rmtree('intermediate_data')\n",
    "        print(\"Intermediate files removed.\")\n",
    "\n",
    "# cleanup_intermediate_files()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your final preprocessed DataFrame\n",
    "import pandas as pd\n",
    "df = pd.read_pickle('final_nlp_data.pkl')  # Adjust path as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "tfidf_matrix = tfidf.fit_transform(df['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Nearest Neighbors\n",
    "nn = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "nn.fit(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommend Function\n",
    "def recommend_articles_nn(title, df=df, top_n=10):\n",
    "    if title not in df['title'].values:\n",
    "        return f\"'{title}' not found.\"\n",
    "\n",
    "    idx = df[df['title'] == title].index[0]\n",
    "    query_vector = tfidf_matrix[idx]\n",
    "\n",
    "    distances, indices = nn.kneighbors(query_vector, n_neighbors=top_n+1)\n",
    "    \n",
    "    # Skip self-match\n",
    "    similar_indices = indices.flatten()[1:]\n",
    "    \n",
    "    return df[['title', 'url']].iloc[similar_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = recommend_articles_nn(\"Mental Note Vol. 24\")\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensure no duplicate titles\n",
    "# df = df.drop_duplicates(subset='title', keep='first').reset_index(drop=True)\n",
    "\n",
    "# # Create mapping\n",
    "# indices = pd.Series(df.index, index=df['title']).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def recommend_articles(title, cosine_sim=cosine_sim, indices=indices, df=df, top_n=10):\n",
    "#     if title not in indices:\n",
    "#         return f\"Article '{title}' not found in dataset.\"\n",
    "\n",
    "#     idx = indices[title]\n",
    "\n",
    "#     # Get pairwise similarity scores for that article\n",
    "#     sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "#     # Sort articles by similarity (highest first)\n",
    "#     sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#     # Skip the first one (it will be the input article itself)\n",
    "#     sim_scores = sim_scores[1:top_n+1]\n",
    "\n",
    "#     # Get the indices of the recommended articles\n",
    "#     article_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "#     # Return top N similar articles\n",
    "#     return df[['title', 'url']].iloc[article_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next word Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = df.sample(n=10000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "all_sentences = []\n",
    "for doc in sampled_df['clean_text']:\n",
    "    all_sentences.extend(sent_tokenize(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_sentences)\n",
    "\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_sentences = all_sentences[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating n-gram sequences:   0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating n-gram sequences: 100%|██████████| 5000/5000 [28:41<00:00,  2.90it/s]   \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "input_sequences = []\n",
    "\n",
    "for line in tqdm(sampled_sentences, desc=\"Generating n-gram sequences\"):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_seq = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save to file\n",
    "with open('intermediate_data/input_sequences.pkl', 'wb') as f:\n",
    "    pickle.dump(input_sequences, f)\n",
    "\n",
    "\n",
    "# Free memory\n",
    "del input_sequences\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mintermediate_data/input_sequences.pkl\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     input_sequences = \u001b[43mpickle\u001b[49m.load(f)\n",
      "\u001b[31mNameError\u001b[39m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "with open(\"intermediate_data/input_sequences.pkl\", \"rb\") as f:\n",
    "    input_sequences = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Pad to same length\n",
    "max_seq_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre')\n",
    "\n",
    "# Split\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n",
    "\n",
    "# One-hot encode labels\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y = to_categorical(y, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_seq_len-1))\n",
    "model.add(LSTM(150, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=10, verbose=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words=20):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        output_word = tokenizer.index_word[np.argmax(predicted)]\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(\"Hello Everyone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
