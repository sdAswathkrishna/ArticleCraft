{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medium Articles NLP Analysis\n",
    "\n",
    "This notebook performs Natural Language Processing on a large dataset of Medium articles. Due to the size of the dataset, we'll implement techniques to handle memory constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "# NLP libraries\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# For memory usage tracking\n",
    "import psutil\n",
    "\n",
    "# # Download necessary NLTK data\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "def print_memory_usage(message=\"\"):\n",
    "    \"\"\"Print memory usage with optional message\"\"\"\n",
    "    memory_mb = get_memory_usage()\n",
    "    print(f\"{message} - Memory usage: {memory_mb:.2f} MB\")\n",
    "    \n",
    "# Create directory for intermediate results if it doesn't exist\n",
    "if not os.path.exists('intermediate_data'):\n",
    "    os.makedirs('intermediate_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from medium_articles.csv...\n",
      "Loaded chunk 1 with 10000 rows\n",
      " - Memory usage: 1180.95 MB\n",
      "Loaded chunk 2 with 10000 rows\n",
      " - Memory usage: 1297.41 MB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Replace 'medium_articles.json' with your actual file path\u001b[39;00m\n\u001b[32m     21\u001b[39m file_path = \u001b[33m'\u001b[39m\u001b[33mmedium_articles.csv\u001b[39m\u001b[33m'\u001b[39m  \n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m data_chunks = \u001b[43mload_dataset_in_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mload_dataset_in_chunks\u001b[39m\u001b[34m(file_path, chunk_size)\u001b[39m\n\u001b[32m      8\u001b[39m total_rows = \u001b[32m0\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Use chunksize to load in batches\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLoaded chunk \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m with \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m rows\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Codrelate 2025\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1843\u001b[39m, in \u001b[36mTextFileReader.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1841\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> DataFrame:\n\u001b[32m   1842\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1843\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1844\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   1845\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Codrelate 2025\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1985\u001b[39m, in \u001b[36mTextFileReader.get_chunk\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m   1983\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[32m   1984\u001b[39m     size = \u001b[38;5;28mmin\u001b[39m(size, \u001b[38;5;28mself\u001b[39m.nrows - \u001b[38;5;28mself\u001b[39m._currow)\n\u001b[32m-> \u001b[39m\u001b[32m1985\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Codrelate 2025\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Codrelate 2025\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:850\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:331\u001b[39m, in \u001b[36mgetstate\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# We'll use chunking to handle large files\n",
    "\n",
    "def load_dataset_in_chunks(file_path, chunk_size=10000):\n",
    "    \"\"\"Load large Csv dataset in chunks\"\"\"\n",
    "    print(f\"Loading dataset from {file_path}...\")\n",
    "    \n",
    "    chunks = []\n",
    "    total_rows = 0\n",
    "    \n",
    "    # Use chunksize to load in batches\n",
    "    for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):\n",
    "        print(f\"Loaded chunk {i+1} with {len(chunk)} rows\")\n",
    "        chunks.append(chunk)\n",
    "        total_rows += len(chunk)\n",
    "        print_memory_usage()\n",
    "    \n",
    "    print(f\"Total rows loaded: {total_rows}\")\n",
    "    return chunks\n",
    "\n",
    "# Replace 'medium_articles.json' with your actual file path\n",
    "file_path = 'medium_articles.csv'  \n",
    "data_chunks = load_dataset_in_chunks(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the first chunk to understand the data structure\n",
    "sample_df = data_chunks[0]\n",
    "\n",
    "# Display basic information\n",
    "print(\"Sample data structure:\")\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataframe info\n",
    "print(\"\\nDataframe info:\")\n",
    "sample_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display statistics for numeric columns (if any)\n",
    "print(\"\\nDescriptive statistics:\")\n",
    "sample_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for column data types and null values across all chunks\n",
    "def analyze_all_chunks(chunks):\n",
    "    total_rows = 0\n",
    "    null_counts = {}\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        total_rows += len(chunk)\n",
    "        \n",
    "        # Count nulls in this chunk\n",
    "        chunk_nulls = chunk.isnull().sum()\n",
    "        \n",
    "        # Update total null counts\n",
    "        for col in chunk.columns:\n",
    "            if col not in null_counts:\n",
    "                null_counts[col] = 0\n",
    "            null_counts[col] += chunk_nulls[col]\n",
    "    \n",
    "    print(f\"Total rows across all chunks: {total_rows}\")\n",
    "    print(\"\\nNull values per column:\")\n",
    "    for col, count in null_counts.items():\n",
    "        percentage = (count / total_rows) * 100\n",
    "        print(f\"{col}: {count} nulls ({percentage:.2f}%)\")\n",
    "\n",
    "analyze_all_chunks(data_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast  # Add this import at the top\n",
    "\n",
    "def convert_data_types(df):\n",
    "    \"\"\"Convert data types for timestamp, authors, and tags\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    if 'timestamp' in df_copy.columns:\n",
    "        df_copy['timestamp'] = pd.to_datetime(df_copy['timestamp'], errors='coerce')\n",
    "\n",
    "    # Helper function to safely evaluate strings into lists\n",
    "    def safe_parse_list(x):\n",
    "        if isinstance(x, str):\n",
    "            try:\n",
    "                return ast.literal_eval(x)\n",
    "            except (ValueError, SyntaxError):\n",
    "                return []\n",
    "        elif isinstance(x, list):\n",
    "            return x\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    # Ensure authors and tags are proper lists\n",
    "    if 'authors' in df_copy.columns:\n",
    "        df_copy['authors'] = df_copy['authors'].apply(safe_parse_list)\n",
    "\n",
    "    if 'tags' in df_copy.columns:\n",
    "        df_copy['tags'] = df_copy['tags'].apply(safe_parse_list)\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Process each chunk and save as intermediate file\n",
    "for i, chunk in enumerate(data_chunks):\n",
    "    print(f\"Converting data types for chunk {i+1}...\")\n",
    "    converted_chunk = convert_data_types(chunk)\n",
    "    \n",
    "    # Save intermediate result\n",
    "    intermediate_file = f'intermediate_data/converted_chunk_{i}.pkl'\n",
    "    converted_chunk.to_pickle(intermediate_file)\n",
    "    print(f\"Saved converted chunk to {intermediate_file}\")\n",
    "    print_memory_usage()\n",
    "\n",
    "# Clear memory\n",
    "del data_chunks\n",
    "gc.collect()\n",
    "print_memory_usage(\"After clearing data chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"Clean data by removing nulls, filling empty lists, and cleaning text\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Remove rows with null values in critical columns\n",
    "    critical_cols = ['title', 'text', 'url']\n",
    "    df_clean = df_clean.dropna(subset=critical_cols)\n",
    "    \n",
    "    # Replace empty lists\n",
    "    if 'authors' in df_clean.columns:\n",
    "        df_clean['authors'] = df_clean['authors'].apply(lambda x: ['Unknown'] if not x else x)\n",
    "    \n",
    "    if 'tags' in df_clean.columns:\n",
    "        df_clean['tags'] = df_clean['tags'].apply(lambda x: ['Untagged'] if not x else x)\n",
    "    \n",
    "    # Clean text columns\n",
    "    def clean_text(text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        \n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()    \n",
    "        \n",
    "        # Remove newline characters\n",
    "        text = re.sub(r'\\n+', ' ', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    # Apply text cleaning to title and text columns\n",
    "    if 'title' in df_clean.columns:\n",
    "        df_clean['clean_title'] = df_clean['title'].apply(clean_text)\n",
    "    \n",
    "    if 'text' in df_clean.columns:\n",
    "        df_clean['clean_text'] = df_clean['text'].apply(clean_text)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Process each converted chunk and save as intermediate file\n",
    "num_chunks = len([f for f in os.listdir('intermediate_data') if f.startswith('converted_chunk_')])\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    # Load the converted chunk\n",
    "    file_path = f'intermediate_data/converted_chunk_{i}.pkl'\n",
    "    print(f\"Loading {file_path}...\")\n",
    "    chunk = pd.read_pickle(file_path)\n",
    "    \n",
    "    # Clean the chunk\n",
    "    print(f\"Cleaning chunk {i+1}...\")\n",
    "    cleaned_chunk = clean_data(chunk)\n",
    "    \n",
    "    # Save cleaned chunk\n",
    "    cleaned_file = f'intermediate_data/cleaned_chunk_{i}.pkl'\n",
    "    cleaned_chunk.to_pickle(cleaned_file)\n",
    "    print(f\"Saved cleaned chunk to {cleaned_file}\")\n",
    "    print_memory_usage()\n",
    "    \n",
    "    # Free memory\n",
    "    del chunk, cleaned_chunk\n",
    "    gc.collect()\n",
    "\n",
    "print_memory_usage(\"After cleaning all chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. NLP Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_for_nlp(df):\n",
    "    \"\"\"Apply NLP preprocessing: tokenization, remove stopwords, lemmatization, stemming\"\"\"\n",
    "    df_nlp = df.copy()\n",
    "    \n",
    "    # Initialize tools\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    def process_text(text):\n",
    "        if not isinstance(text, str) or not text:\n",
    "            return [], [], []\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "        \n",
    "        # Lemmatize\n",
    "        lemmatized = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "        \n",
    "        # Stem\n",
    "        stemmed = [stemmer.stem(word) for word in filtered_tokens]\n",
    "        \n",
    "        return filtered_tokens, lemmatized, stemmed\n",
    "    \n",
    "    # Process text and title\n",
    "    if 'clean_text' in df_nlp.columns:\n",
    "        print(\"Processing text column...\")\n",
    "        # Process in batches to avoid memory issues\n",
    "        batch_size = 1000\n",
    "        tokens_list = []\n",
    "        lemmatized_list = []\n",
    "        stemmed_list = []\n",
    "        \n",
    "        for i in range(0, len(df_nlp), batch_size):\n",
    "            print(f\"Processing batch {i//batch_size + 1}...\")\n",
    "            batch = df_nlp['clean_text'].iloc[i:i+batch_size]\n",
    "            \n",
    "            batch_results = [process_text(text) for text in batch]\n",
    "            \n",
    "            # Unpack results\n",
    "            batch_tokens, batch_lemmatized, batch_stemmed = zip(*batch_results)\n",
    "            \n",
    "            tokens_list.extend(batch_tokens)\n",
    "            lemmatized_list.extend(batch_lemmatized)\n",
    "            stemmed_list.extend(batch_stemmed)\n",
    "            \n",
    "            # Print memory usage after each batch\n",
    "            if (i//batch_size) % 10 == 0:\n",
    "                print_memory_usage(f\"After processing batch {i//batch_size + 1}\")\n",
    "        \n",
    "        # Add results to dataframe\n",
    "        df_nlp['tokens'] = tokens_list\n",
    "        df_nlp['lemmatized'] = lemmatized_list\n",
    "        df_nlp['stemmed'] = stemmed_list\n",
    "    \n",
    "    # Process title (simpler as titles are shorter)\n",
    "    if 'clean_title' in df_nlp.columns:\n",
    "        print(\"Processing title column...\")\n",
    "        title_results = [process_text(title) for title in df_nlp['clean_title']]\n",
    "        title_tokens, title_lemmatized, title_stemmed = zip(*title_results)\n",
    "        \n",
    "        df_nlp['title_tokens'] = list(title_tokens)\n",
    "        df_nlp['title_lemmatized'] = list(title_lemmatized)\n",
    "        df_nlp['title_stemmed'] = list(title_stemmed)\n",
    "    \n",
    "    return df_nlp\n",
    "\n",
    "# Process each cleaned chunk for NLP\n",
    "num_chunks = len([f for f in os.listdir('intermediate_data') if f.startswith('cleaned_chunk_')])\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    # Load the cleaned chunk\n",
    "    file_path = f'intermediate_data/cleaned_chunk_{i}.pkl'\n",
    "    print(f\"Loading {file_path}...\")\n",
    "    chunk = pd.read_pickle(file_path)\n",
    "    \n",
    "    # Preprocess for NLP\n",
    "    print(f\"Preprocessing chunk {i+1} for NLP...\")\n",
    "    nlp_chunk = preprocess_text_for_nlp(chunk)\n",
    "    \n",
    "    # Save NLP preprocessed chunk\n",
    "    nlp_file = f'intermediate_data/nlp_chunk_{i}.pkl'\n",
    "    nlp_chunk.to_pickle(nlp_file)\n",
    "    print(f\"Saved NLP preprocessed chunk to {nlp_file}\")\n",
    "    print_memory_usage()\n",
    "    \n",
    "    # Free memory\n",
    "    del chunk, nlp_chunk\n",
    "    gc.collect()\n",
    "\n",
    "print_memory_usage(\"After NLP preprocessing all chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Word Count Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate word count statistics across all chunks\n",
    "def word_count_analysis():\n",
    "    num_chunks = len([f for f in os.listdir('intermediate_data') if f.startswith('nlp_chunk_')])\n",
    "    \n",
    "    # Initialize stats\n",
    "    title_lengths = []\n",
    "    text_lengths = []\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        # Load chunk\n",
    "        file_path = f'intermediate_data/nlp_chunk_{i}.pkl'\n",
    "        print(f\"Analyzing word counts in {file_path}...\")\n",
    "        chunk = pd.read_pickle(file_path)\n",
    "        \n",
    "        # Calculate lengths\n",
    "        if 'tokens' in chunk.columns:\n",
    "            chunk_text_lengths = chunk['tokens'].apply(len)\n",
    "            text_lengths.extend(chunk_text_lengths)\n",
    "        \n",
    "        if 'title_tokens' in chunk.columns:\n",
    "            chunk_title_lengths = chunk['title_tokens'].apply(len)\n",
    "            title_lengths.extend(chunk_title_lengths)\n",
    "        \n",
    "        # Free memory\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Convert to Series for analysis\n",
    "    title_lengths = pd.Series(title_lengths)\n",
    "    text_lengths = pd.Series(text_lengths)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    print(\"\\nTitle word count statistics:\")\n",
    "    print(title_lengths.describe())\n",
    "    \n",
    "    print(\"\\nArticle text word count statistics:\")\n",
    "    print(text_lengths.describe())\n",
    "    \n",
    "    # Create histograms\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(title_lengths, kde=True)\n",
    "    plt.title('Distribution of Title Word Counts')\n",
    "    plt.xlabel('Word Count')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(text_lengths.clip(upper=1000), kde=True)  # Clip to avoid extreme outliers\n",
    "    plt.title('Distribution of Article Word Counts (capped at 1000)')\n",
    "    plt.xlabel('Word Count')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('word_count_distribution.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return title_lengths, text_lengths\n",
    "\n",
    "title_lengths, text_lengths = word_count_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_common_words(field='lemmatized', n=30):\n",
    "    \"\"\"Find most common words across all chunks for a given field\"\"\"\n",
    "    num_chunks = len([f for f in os.listdir('intermediate_data') if f.startswith('nlp_chunk_')])\n",
    "    \n",
    "    # Initialize counter\n",
    "    word_counter = Counter()\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        # Load chunk\n",
    "        file_path = f'intermediate_data/nlp_chunk_{i}.pkl'\n",
    "        print(f\"Finding common words in {file_path}...\")\n",
    "        chunk = pd.read_pickle(file_path)\n",
    "        \n",
    "        # Count words\n",
    "        if field in chunk.columns:\n",
    "            # Flatten list of lists and count\n",
    "            words = [word for word_list in chunk[field] for word in word_list if len(word) > 1]\n",
    "            word_counter.update(words)\n",
    "        \n",
    "        # Free memory\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Get most common words\n",
    "    most_common = word_counter.most_common(n)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    words, counts = zip(*most_common)\n",
    "    sns.barplot(x=list(counts), y=list(words))\n",
    "    plt.title(f'Top {n} Most Common Words')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'most_common_{field}.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(most_common))\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Word Cloud of Top Words')\n",
    "    plt.savefig(f'wordcloud_{field}.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return most_common\n",
    "\n",
    "# Find most common lemmatized words\n",
    "most_common_words = find_most_common_words('lemmatized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Most Common Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_common_tags(n=20):\n",
    "    \"\"\"Find most common tags across all chunks\"\"\"\n",
    "    num_chunks = len([f for f in os.listdir('intermediate_data') if f.startswith('nlp_chunk_')])\n",
    "    \n",
    "    # Initialize counter\n",
    "    tag_counter = Counter()\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        # Load chunk\n",
    "        file_path = f'intermediate_data/nlp_chunk_{i}.pkl'\n",
    "        print(f\"Finding common tags in {file_path}...\")\n",
    "        chunk = pd.read_pickle(file_path)\n",
    "        \n",
    "        # Count tags\n",
    "        if 'tags' in chunk.columns:\n",
    "            # Flatten list of lists and count\n",
    "            tags = [tag for tag_list in chunk['tags'] for tag in tag_list]\n",
    "            tag_counter.update(tags)\n",
    "        \n",
    "        # Free memory\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Get most common tags\n",
    "    most_common = tag_counter.most_common(n)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    tags, counts = zip(*most_common)\n",
    "    sns.barplot(x=list(counts), y=list(tags))\n",
    "    plt.title(f'Top {n} Most Common Tags')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('most_common_tags.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return most_common\n",
    "\n",
    "# Find most common tags\n",
    "most_common_tags = find_most_common_tags()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Authors with Most Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_prolific_authors(n=20):\n",
    "    \"\"\"Find authors with the most articles across all chunks\"\"\"\n",
    "    num_chunks = len([f for f in os.listdir('intermediate_data') if f.startswith('nlp_chunk_')])\n",
    "    \n",
    "    # Initialize counter\n",
    "    author_counter = Counter()\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        # Load chunk\n",
    "        file_path = f'intermediate_data/nlp_chunk_{i}.pkl'\n",
    "        print(f\"Analyzing authors in {file_path}...\")\n",
    "        chunk = pd.read_pickle(file_path)\n",
    "        \n",
    "        # Count authors\n",
    "        if 'authors' in chunk.columns:\n",
    "            # Flatten list of lists and count\n",
    "            authors = [author for author_list in chunk['authors'] for author in author_list]\n",
    "            author_counter.update(authors)\n",
    "        \n",
    "        # Free memory\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Get most prolific authors\n",
    "    most_prolific = author_counter.most_common(n)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    authors, counts = zip(*most_prolific)\n",
    "    sns.barplot(x=list(counts), y=list(authors))\n",
    "    plt.title(f'Top {n} Most Prolific Authors')\n",
    "    plt.xlabel('Number of Articles')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('most_prolific_authors.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return most_prolific\n",
    "\n",
    "# Find most prolific authors\n",
    "most_prolific_authors = find_most_prolific_authors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Most Frequent N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(tokens_list, n=2):\n",
    "    \"\"\"Generate n-grams from a list of tokens\"\"\"\n",
    "    ngrams = []\n",
    "    for tokens in tokens_list:\n",
    "        if len(tokens) >= n:\n",
    "            # Generate n-grams\n",
    "            grams = [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "            ngrams.extend(grams)\n",
    "    return ngrams\n",
    "\n",
    "def find_most_common_ngrams(n_gram=2, top_n=20):\n",
    "    \"\"\"Find most common n-grams across all chunks\"\"\"\n",
    "    num_chunks = len([f for f in os.listdir('intermediate_data') if f.startswith('nlp_chunk_')])\n",
    "    \n",
    "    # Initialize counter\n",
    "    ngram_counter = Counter()\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        # Load chunk\n",
    "        file_path = f'intermediate_data/nlp_chunk_{i}.pkl'\n",
    "        print(f\"Analyzing {n_gram}-grams in {file_path}...\")\n",
    "        chunk = pd.read_pickle(file_path)\n",
    "        \n",
    "        # Generate and count n-grams\n",
    "        if 'lemmatized' in chunk.columns:\n",
    "            chunk_ngrams = generate_ngrams(chunk['lemmatized'], n=n_gram)\n",
    "            ngram_counter.update(chunk_ngrams)\n",
    "        \n",
    "        # Free memory\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Get most common n-grams\n",
    "    most_common = ngram_counter.most_common(top_n)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ngrams, counts = zip(*most_common)\n",
    "    sns.barplot(x=list(counts), y=list(ngrams))\n",
    "    plt.title(f'Top {top_n} Most Common {n_gram}-grams')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'most_common_{n_gram}grams.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return most_common\n",
    "\n",
    "# Find most common bi-grams and tri-grams\n",
    "most_common_bigrams = find_most_common_ngrams(n_gram=2)\n",
    "most_common_trigrams = find_most_common_ngrams(n_gram=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Publication Trends Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_publication_trends():\n",
    "    \"\"\"Analyze publication trends over time\"\"\"\n",
    "    num_chunks = len([f for f in os.listdir('intermediate_data') if f.startswith('nlp_chunk_')])\n",
    "    \n",
    "    # Initialize lists to store timestamps\n",
    "    timestamps = []\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        # Load chunk\n",
    "        file_path = f'intermediate_data/nlp_chunk_{i}.pkl'\n",
    "        print(f\"Analyzing timestamps in {file_path}...\")\n",
    "        chunk = pd.read_pickle(file_path)\n",
    "        \n",
    "        # Extract timestamps\n",
    "        if 'timestamp' in chunk.columns:\n",
    "            timestamps.extend(chunk['timestamp'].dropna())\n",
    "        \n",
    "        # Free memory\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    timestamps_df = pd.DataFrame({'timestamp': timestamps})\n",
    "    \n",
    "    # Extract date components\n",
    "    timestamps_df['year'] = timestamps_df['timestamp'].dt.year\n",
    "    timestamps_df['month'] = timestamps_df['timestamp'].dt.month\n",
    "    timestamps_df['day'] = timestamps_df['timestamp'].dt.day\n",
    "    timestamps_df['hour'] = timestamps_df['timestamp'].dt.hour\n",
    "    timestamps_df['weekday'] = timestamps_df['timestamp'].dt.weekday\n",
    "    \n",
    "    # Create year-month column for trend analysis\n",
    "    timestamps_df['year_month'] = timestamps_df['timestamp'].dt.to_period('M')\n",
    "    \n",
    "    # Monthly publication counts\n",
    "    monthly_counts = timestamps_df['year_month'].value_counts().sort_index()\n",
    "    monthly_counts = monthly_counts.reset_index()\n",
    "    monthly_counts.columns = ['Month', 'Count']\n",
    "    \n",
    "    # Plot monthly trends\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(monthly_counts['Month'].astype(str), monthly_counts['Count'])\n",
    "    plt.title('Monthly Publication Trends')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Number of Articles')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('monthly_publication_trends.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Publication by day of week\n",
    "    weekday_counts = timestamps_df['weekday'].value_counts().sort_index()\n",
    "    weekday_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(weekday_names, weekday_counts)\n",
    "    plt.title('Publication by Day of Week')\n",
    "    plt.xlabel('Day of Week')\n",
    "    plt.ylabel('Number of Articles')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('weekday_publication_trends.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Publication by hour of day\n",
    "    hour_counts = timestamps_df['hour'].value_counts().sort_index()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(hour_counts.index, hour_counts.values)\n",
    "    plt.title('Publication by Hour of Day')\n",
    "    plt.xlabel('Hour (24-hour format)')\n",
    "    plt.ylabel('Number of Articles')\n",
    "    plt.xticks(range(0, 24))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('hourly_publication_trends.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return timestamps_df\n",
    "\n",
    "# Analyze publication trends\n",
    "time_analysis = analyze_publication_trends()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section requires additional memory and processing power\n",
    "# Uncomment and run if your system can handle it\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def perform_topic_modeling(n_topics=10, n_top_words=15):\n",
    "    \"\"\"Perform topic modeling using LDA\"\"\"\n",
    "    # This requires loading all data into memory\n",
    "    # Consider using a subset if memory is limited\n",
    "    \n",
    "    # Concatenate all lemmatized tokens into documents\n",
    "    documents = []\n",
    "    \n",
    "    num_chunks = len([f for f in os.listdir('intermediate_data') if f.startswith('nlp_chunk_')])\n",
    "    \n",
    "    # Use a subset if data is too large\n",
    "    max_docs = 10000  # Adjust based on memory constraints\n",
    "    doc_count = 0\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        if doc_count >= max_docs:\n",
    "            break\n",
    "            \n",
    "        # Load chunk\n",
    "        file_path = f'intermediate_data/nlp_chunk_{i}.pkl'\n",
    "        print(f\"Loading {file_path} for topic modeling...\")\n",
    "        chunk = pd.read_pickle(file_path)\n",
    "        \n",
    "        # Convert lemmatized tokens to documents\n",
    "        if 'lemmatized' in chunk.columns:\n",
    "            chunk_docs = [' '.join(tokens) for tokens in chunk['lemmatized']]\n",
    "            documents.extend(chunk_docs[:max_docs-doc_count])\n",
    "            doc_count += len(chunk_docs[:max_docs-doc_count])\n",
    "        \n",
    "        # Free memory\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"Performing topic modeling on {len(documents)} documents...\")\n",
    "    \n",
    "    # Create document-term matrix\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=10000)\n",
    "    dtm = vectorizer.fit_transform(documents)\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Fit LDA model\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics,\n",
    "        random_state=42,\n",
    "        max_iter=5,  # Reduce for memory constraints\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    lda.fit(dtm)\n",
    "    \n",
    "    # Print topics\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        print(f\"Topic #{topic_idx+1}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words-1:-1]]))\n",
    "        print()\n",
    "    \n",
    "    return lda, vectorizer, feature_names\n",
    "\n",
    "# Perform topic modeling\n",
    "lda_model, vectorizer, feature_names = perform_topic_modeling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save analysis results\n",
    "results = {\n",
    "    'most_common_words': most_common_words,\n",
    "    'most_common_tags': most_common_tags,\n",
    "    'most_prolific_authors': most_prolific_authors,\n",
    "    'most_common_bigrams': most_common_bigrams,\n",
    "    'most_common_trigrams': most_common_trigrams\n",
    "}\n",
    "\n",
    "# Save results as pickle\n",
    "with open('analysis_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "print(\"Analysis results saved to 'analysis_results.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NLP Analysis of Medium Articles - Summary\")\n",
    "print(\"==========================================\\n\")\n",
    "\n",
    "# Load results\n",
    "with open('analysis_results.pkl', 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "print(\"Top 10 Most Common Words:\")\n",
    "for word, count in results['most_common_words'][:10]:\n",
    "    print(f\"  {word}: {count}\")\n",
    "print()\n",
    "\n",
    "print(\"Top 10 Most Common Tags:\")\n",
    "for tag, count in results['most_common_tags'][:10]:\n",
    "    print(f\"  {tag}: {count}\")\n",
    "print()\n",
    "\n",
    "print(\"Top 10 Most Prolific Authors:\")\n",
    "for author, count in results['most_prolific_authors'][:10]:\n",
    "    print(f\"  {author}: {count} articles\")\n",
    "print()\n",
    "\n",
    "print(\"Top 10 Most Common Bigrams:\")\n",
    "for bigram, count in results['most_common_bigrams'][:10]:\n",
    "    print(f\"  {bigram}: {count}\")\n",
    "print()\n",
    "\n",
    "print(\"Top 10 Most Common Trigrams:\")\n",
    "for trigram, count in results['most_common_trigrams'][:10]:\n",
    "    print(f\"  {trigram}: {count}\")\n",
    "print()\n",
    "\n",
    "print(\"Analysis completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where your intermediate chunks are stored\n",
    "intermediate_folder = 'intermediate_data'\n",
    "\n",
    "# Identify all NLP processed chunk files\n",
    "chunk_files = sorted([\n",
    "    f for f in os.listdir(intermediate_folder)\n",
    "    if f.startswith('nlp_chunk_') and f.endswith('.pkl')\n",
    "])\n",
    "\n",
    "print(f\"Found {len(chunk_files)} NLP chunks to merge.\")\n",
    "\n",
    "# Initialize list to collect DataFrames\n",
    "all_chunks = []\n",
    "\n",
    "# Load and append each chunk\n",
    "for i, file in enumerate(chunk_files):\n",
    "    file_path = os.path.join(intermediate_folder, file)\n",
    "    print(f\"Loading {file_path}...\")\n",
    "    \n",
    "    chunk_df = pd.read_pickle(file_path)\n",
    "    all_chunks.append(chunk_df)\n",
    "    \n",
    "    # Clear memory from last chunk\n",
    "    del chunk_df\n",
    "    gc.collect()\n",
    "\n",
    "# Concatenate all chunks\n",
    "final_df = pd.concat(all_chunks, ignore_index=True)\n",
    "print(f\"Final merged DataFrame shape: {final_df.shape}\")\n",
    "\n",
    "# Optional: drop duplicate articles if needed\n",
    "# final_df.drop_duplicates(subset=['title', 'url'], inplace=True)\n",
    "\n",
    "# Save the final dataset as a pickle (fast & preserves Python objects)\n",
    "final_df.to_pickle('final_nlp_data.pkl')\n",
    "print(\"Saved final merged data to 'final_nlp_data.pkl'.\")\n",
    "\n",
    "# Optionally also save as CSV (if you want to inspect or use outside Python)\n",
    "final_df.to_csv('final_nlp_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clean up intermediate files\n",
    "'''\n",
    "import shutil\n",
    "\n",
    "def cleanup_intermediate_files():\n",
    "    \"\"\"Remove intermediate files to free up disk space\"\"\"\n",
    "    print(\"Cleaning up intermediate files...\")\n",
    "    if os.path.exists('intermediate_data'):\n",
    "        shutil.rmtree('intermediate_data')\n",
    "        print(\"Intermediate files removed.\")\n",
    "\n",
    "# cleanup_intermediate_files()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
